{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30152,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\nThis project focuses on implementing and evaluating an offline reinforcement learning (RL) algorithm using the Hopper environment from the MuJoCo suite Gym MuJoCo documentation, which is part of the D4RL library. The Implicit Q-Learning (IQL) algorithm has been implemen ted in PyTorch and tested on the Hopper environment. This project also includes logging and experiment tracking using Weights & Biases (wandb).","metadata":{}},{"cell_type":"markdown","source":"# Install Mujuco Physics engine on the kernel","metadata":{}},{"cell_type":"code","source":"import os\nif not os.path.exists('.mujoco_setup_complete'):\n  # Get the prereqs\n  !apt-get -qq update\n  !apt-get -qq install -y libosmesa6-dev libgl1-mesa-glx libglfw3 libgl1-mesa-dev libglew-dev patchelf\n  # Get Mujoco\n  !mkdir ~/.mujoco\n  !wget -q https://mujoco.org/download/mujoco210-linux-x86_64.tar.gz -O mujoco.tar.gz\n  !tar -zxf mujoco.tar.gz -C \"$HOME/.mujoco\"\n  !rm mujoco.tar.gz\n  # Add it to the actively loaded path and the bashrc path (these only do so much)\n  !echo 'export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$HOME/.mujoco/mujoco210/bin' >> ~/.bashrc \n  !echo 'export LD_PRELOAD=$LD_PRELOAD:/usr/lib/x86_64-linux-gnu/libGLEW.so' >> ~/.bashrc \n  # THE ANNOYING ONE, FORCE IT INTO LDCONFIG SO WE ACTUALLY GET ACCESS TO IT THIS SESSION\n  !echo \"/root/.mujoco/mujoco210/bin\" > /etc/ld.so.conf.d/mujoco_ld_lib_path.conf\n  !ldconfig\n  # Install Mujoco-py\n  !pip3 install -U 'mujoco-py<2.2,>=2.1'\n  # run once\n  !touch .mujoco_setup_complete\n\ntry:\n  if _mujoco_run_once:\n    pass\nexcept NameError:\n  _mujoco_run_once = False\nif not _mujoco_run_once:\n  # Add it to the actively loaded path and the bashrc path (these only do so much)\n  try:\n    os.environ['LD_LIBRARY_PATH']=os.environ['LD_LIBRARY_PATH'] + ':/root/.mujoco/mujoco210/bin'\n  except KeyError:\n    os.environ['LD_LIBRARY_PATH']='/root/.mujoco/mujoco210/bin'\n  try:\n    os.environ['LD_PRELOAD']=os.environ['LD_PRELOAD'] + ':/usr/lib/x86_64-linux-gnu/libGLEW.so'\n  except KeyError:\n    os.environ['LD_PRELOAD']='/usr/lib/x86_64-linux-gnu/libGLEW.so'\n  # presetup so we don't see output on first env initialization\n  import mujoco_py\n  _mujoco_run_once = True\n#source of this code block : https://gist.github.com/BuildingAtom/3119ac9c595324c8001a7454f23bf8c8","metadata":{"execution":{"iopub.status.busy":"2024-08-14T19:36:35.429674Z","iopub.execute_input":"2024-08-14T19:36:35.430019Z","iopub.status.idle":"2024-08-14T19:36:35.495342Z","shell.execute_reply.started":"2024-08-14T19:36:35.429980Z","shell.execute_reply":"2024-08-14T19:36:35.494426Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"Then import it :","metadata":{}},{"cell_type":"code","source":"import mujoco_py","metadata":{"execution":{"iopub.status.busy":"2024-08-14T19:36:49.053046Z","iopub.execute_input":"2024-08-14T19:36:49.053911Z","iopub.status.idle":"2024-08-14T19:36:49.057947Z","shell.execute_reply.started":"2024-08-14T19:36:49.053863Z","shell.execute_reply":"2024-08-14T19:36:49.057046Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## install D4RL","metadata":{}},{"cell_type":"code","source":"# intall d4rl github repo\n!pip install git+https://github.com/Farama-Foundation/d4rl@master#egg=d4rl","metadata":{"execution":{"iopub.status.busy":"2024-08-14T19:36:51.377311Z","iopub.execute_input":"2024-08-14T19:36:51.378020Z","iopub.status.idle":"2024-08-14T19:37:04.990465Z","shell.execute_reply.started":"2024-08-14T19:36:51.377977Z","shell.execute_reply":"2024-08-14T19:37:04.989450Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Collecting d4rl\n  Cloning https://github.com/Farama-Foundation/d4rl (to revision master) to /tmp/pip-install-fxso1vfe/d4rl_3a4db1e4421347c1990223737cf55404\n  Running command git clone --filter=blob:none -q https://github.com/Farama-Foundation/d4rl /tmp/pip-install-fxso1vfe/d4rl_3a4db1e4421347c1990223737cf55404\n  Resolved https://github.com/Farama-Foundation/d4rl to commit 71a9549f2091accff93eeff68f1f3ab2c0e0a288\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting mjrl@ git+https://github.com/aravindr93/mjrl@master#egg=mjrl\n  Cloning https://github.com/aravindr93/mjrl (to revision master) to /tmp/pip-install-fxso1vfe/mjrl_bdcf91f7a8f14d29890fc33b3d8be13f\n  Running command git clone --filter=blob:none -q https://github.com/aravindr93/mjrl /tmp/pip-install-fxso1vfe/mjrl_bdcf91f7a8f14d29890fc33b3d8be13f\n  Resolved https://github.com/aravindr93/mjrl to commit 3871d93763d3b49c4741e6daeaebbc605fe140dc\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: gym<0.24.0 in /opt/conda/lib/python3.7/site-packages (from d4rl) (0.21.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from d4rl) (1.19.5)\nRequirement already satisfied: mujoco_py in /opt/conda/lib/python3.7/site-packages (from d4rl) (2.1.2.14)\nRequirement already satisfied: pybullet in /opt/conda/lib/python3.7/site-packages (from d4rl) (3.2.6)\nRequirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (from d4rl) (3.1.0)\nRequirement already satisfied: termcolor in /opt/conda/lib/python3.7/site-packages (from d4rl) (1.1.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from d4rl) (8.0.3)\nRequirement already satisfied: dm_control>=1.0.3 in /opt/conda/lib/python3.7/site-packages (from d4rl) (1.0.13)\nRequirement already satisfied: setuptools!=50.0.0 in /opt/conda/lib/python3.7/site-packages (from dm_control>=1.0.3->d4rl) (59.1.1)\nRequirement already satisfied: mujoco>=2.3.6 in /opt/conda/lib/python3.7/site-packages (from dm_control>=1.0.3->d4rl) (2.3.6)\nRequirement already satisfied: glfw in /opt/conda/lib/python3.7/site-packages (from dm_control>=1.0.3->d4rl) (2.7.0)\nRequirement already satisfied: dm-env in /opt/conda/lib/python3.7/site-packages (from dm_control>=1.0.3->d4rl) (1.6)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from dm_control>=1.0.3->d4rl) (1.7.2)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.7/site-packages (from dm_control>=1.0.3->d4rl) (4.6.4)\nRequirement already satisfied: pyopengl>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from dm_control>=1.0.3->d4rl) (3.1.7)\nRequirement already satisfied: labmaze in /opt/conda/lib/python3.7/site-packages (from dm_control>=1.0.3->d4rl) (1.0.6)\nRequirement already satisfied: absl-py>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from dm_control>=1.0.3->d4rl) (0.15.0)\nRequirement already satisfied: pyparsing>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from dm_control>=1.0.3->d4rl) (3.0.6)\nRequirement already satisfied: dm-tree!=0.1.2 in /opt/conda/lib/python3.7/site-packages (from dm_control>=1.0.3->d4rl) (0.1.6)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from dm_control>=1.0.3->d4rl) (4.62.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from dm_control>=1.0.3->d4rl) (2.25.1)\nRequirement already satisfied: protobuf>=3.19.4 in /opt/conda/lib/python3.7/site-packages (from dm_control>=1.0.3->d4rl) (3.20.0)\nRequirement already satisfied: importlib-metadata>=4.8.1 in /opt/conda/lib/python3.7/site-packages (from gym<0.24.0->d4rl) (4.8.2)\nRequirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from gym<0.24.0->d4rl) (2.0.0)\nRequirement already satisfied: cached-property in /opt/conda/lib/python3.7/site-packages (from h5py->d4rl) (1.5.2)\nRequirement already satisfied: fasteners~=0.15 in /opt/conda/lib/python3.7/site-packages (from mujoco_py->d4rl) (0.16.3)\nRequirement already satisfied: cffi>=1.10 in /opt/conda/lib/python3.7/site-packages (from mujoco_py->d4rl) (1.15.0)\nRequirement already satisfied: Cython>=0.27.2 in /opt/conda/lib/python3.7/site-packages (from mujoco_py->d4rl) (0.29.24)\nRequirement already satisfied: imageio>=2.1.2 in /opt/conda/lib/python3.7/site-packages (from mujoco_py->d4rl) (2.9.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from absl-py>=0.7.0->dm_control>=1.0.3->d4rl) (1.16.0)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.10->mujoco_py->d4rl) (2.21)\nRequirement already satisfied: pillow in /opt/conda/lib/python3.7/site-packages (from imageio>=2.1.2->mujoco_py->d4rl) (8.2.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.8.1->gym<0.24.0->d4rl) (3.6.0)\nRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.8.1->gym<0.24.0->d4rl) (3.10.0.2)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->dm_control>=1.0.3->d4rl) (2.10)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->dm_control>=1.0.3->d4rl) (2021.10.8)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->dm_control>=1.0.3->d4rl) (4.0.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->dm_control>=1.0.3->d4rl) (1.26.7)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## check all the avilable environments","metadata":{}},{"cell_type":"code","source":"import gym\nimport d4rl\n\n# List all available environments to see if Maze2D is included\nenvs = gym.envs.registry.all()\nenv_names = [env_spec.id for env_spec in envs]\nprint(\"Available environments:\", env_names)","metadata":{"execution":{"iopub.status.busy":"2024-08-14T19:37:08.574246Z","iopub.execute_input":"2024-08-14T19:37:08.574584Z","iopub.status.idle":"2024-08-14T19:37:08.581027Z","shell.execute_reply.started":"2024-08-14T19:37:08.574545Z","shell.execute_reply":"2024-08-14T19:37:08.580113Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Available environments: ['CartPole-v0', 'CartPole-v1', 'MountainCar-v0', 'MountainCarContinuous-v0', 'Pendulum-v1', 'Acrobot-v1', 'LunarLander-v2', 'LunarLanderContinuous-v2', 'BipedalWalker-v3', 'BipedalWalkerHardcore-v3', 'CarRacing-v0', 'Blackjack-v1', 'FrozenLake-v1', 'FrozenLake8x8-v1', 'CliffWalking-v0', 'Taxi-v3', 'Reacher-v2', 'Pusher-v2', 'Thrower-v2', 'Striker-v2', 'InvertedPendulum-v2', 'InvertedDoublePendulum-v2', 'HalfCheetah-v2', 'HalfCheetah-v3', 'Hopper-v2', 'Hopper-v3', 'Swimmer-v2', 'Swimmer-v3', 'Walker2d-v2', 'Walker2d-v3', 'Ant-v2', 'Ant-v3', 'Humanoid-v2', 'Humanoid-v3', 'HumanoidStandup-v2', 'FetchSlide-v1', 'FetchPickAndPlace-v1', 'FetchReach-v1', 'FetchPush-v1', 'HandReach-v0', 'HandManipulateBlockRotateZ-v0', 'HandManipulateBlockRotateZTouchSensors-v0', 'HandManipulateBlockRotateZTouchSensors-v1', 'HandManipulateBlockRotateParallel-v0', 'HandManipulateBlockRotateParallelTouchSensors-v0', 'HandManipulateBlockRotateParallelTouchSensors-v1', 'HandManipulateBlockRotateXYZ-v0', 'HandManipulateBlockRotateXYZTouchSensors-v0', 'HandManipulateBlockRotateXYZTouchSensors-v1', 'HandManipulateBlockFull-v0', 'HandManipulateBlock-v0', 'HandManipulateBlockTouchSensors-v0', 'HandManipulateBlockTouchSensors-v1', 'HandManipulateEggRotate-v0', 'HandManipulateEggRotateTouchSensors-v0', 'HandManipulateEggRotateTouchSensors-v1', 'HandManipulateEggFull-v0', 'HandManipulateEgg-v0', 'HandManipulateEggTouchSensors-v0', 'HandManipulateEggTouchSensors-v1', 'HandManipulatePenRotate-v0', 'HandManipulatePenRotateTouchSensors-v0', 'HandManipulatePenRotateTouchSensors-v1', 'HandManipulatePenFull-v0', 'HandManipulatePen-v0', 'HandManipulatePenTouchSensors-v0', 'HandManipulatePenTouchSensors-v1', 'FetchSlideDense-v1', 'FetchPickAndPlaceDense-v1', 'FetchReachDense-v1', 'FetchPushDense-v1', 'HandReachDense-v0', 'HandManipulateBlockRotateZDense-v0', 'HandManipulateBlockRotateZTouchSensorsDense-v0', 'HandManipulateBlockRotateZTouchSensorsDense-v1', 'HandManipulateBlockRotateParallelDense-v0', 'HandManipulateBlockRotateParallelTouchSensorsDense-v0', 'HandManipulateBlockRotateParallelTouchSensorsDense-v1', 'HandManipulateBlockRotateXYZDense-v0', 'HandManipulateBlockRotateXYZTouchSensorsDense-v0', 'HandManipulateBlockRotateXYZTouchSensorsDense-v1', 'HandManipulateBlockFullDense-v0', 'HandManipulateBlockDense-v0', 'HandManipulateBlockTouchSensorsDense-v0', 'HandManipulateBlockTouchSensorsDense-v1', 'HandManipulateEggRotateDense-v0', 'HandManipulateEggRotateTouchSensorsDense-v0', 'HandManipulateEggRotateTouchSensorsDense-v1', 'HandManipulateEggFullDense-v0', 'HandManipulateEggDense-v0', 'HandManipulateEggTouchSensorsDense-v0', 'HandManipulateEggTouchSensorsDense-v1', 'HandManipulatePenRotateDense-v0', 'HandManipulatePenRotateTouchSensorsDense-v0', 'HandManipulatePenRotateTouchSensorsDense-v1', 'HandManipulatePenFullDense-v0', 'HandManipulatePenDense-v0', 'HandManipulatePenTouchSensorsDense-v0', 'HandManipulatePenTouchSensorsDense-v1', 'CubeCrash-v0', 'CubeCrashSparse-v0', 'CubeCrashScreenBecomesBlack-v0', 'MemorizeDigits-v0', 'antmaze-umaze-v0', 'antmaze-umaze-diverse-v0', 'antmaze-medium-play-v0', 'antmaze-medium-diverse-v0', 'antmaze-large-diverse-v0', 'antmaze-large-play-v0', 'antmaze-umaze-v1', 'antmaze-umaze-diverse-v1', 'antmaze-medium-play-v1', 'antmaze-medium-diverse-v1', 'antmaze-large-diverse-v1', 'antmaze-large-play-v1', 'antmaze-eval-umaze-v0', 'antmaze-eval-umaze-diverse-v0', 'antmaze-eval-medium-play-v0', 'antmaze-eval-medium-diverse-v0', 'antmaze-eval-large-diverse-v0', 'antmaze-eval-large-play-v0', 'antmaze-umaze-v2', 'antmaze-umaze-diverse-v2', 'antmaze-medium-play-v2', 'antmaze-medium-diverse-v2', 'antmaze-large-diverse-v2', 'antmaze-large-play-v2', 'mjrl_point_mass-v0', 'mjrl_swimmer-v0', 'mjrl_reacher_7dof-v0', 'mjrl_peg_insertion-v0', 'hammer-human-v1', 'hammer-human-longhorizon-v1', 'hammer-expert-v1', 'hammer-cloned-v1', 'pen-human-v1', 'pen-human-longhorizon-v1', 'pen-expert-v1', 'pen-cloned-v1', 'relocate-human-v1', 'relocate-human-longhorizon-v1', 'relocate-expert-v1', 'relocate-cloned-v1', 'door-human-v1', 'door-human-longhorizon-v1', 'door-expert-v1', 'door-cloned-v1', 'door-v0', 'door-human-v0', 'door-human-longhorizon-v0', 'door-cloned-v0', 'door-expert-v0', 'hammer-v0', 'hammer-human-v0', 'hammer-human-longhorizon-v0', 'hammer-cloned-v0', 'hammer-expert-v0', 'pen-v0', 'pen-human-v0', 'pen-human-longhorizon-v0', 'pen-cloned-v0', 'pen-expert-v0', 'relocate-v0', 'relocate-human-v0', 'relocate-human-longhorizon-v0', 'relocate-cloned-v0', 'relocate-expert-v0', 'maze2d-open-v0', 'maze2d-umaze-v0', 'maze2d-medium-v0', 'maze2d-large-v0', 'maze2d-umaze-v1', 'maze2d-medium-v1', 'maze2d-large-v1', 'maze2d-eval-umaze-v1', 'maze2d-eval-medium-v1', 'maze2d-eval-large-v1', 'maze2d-open-dense-v0', 'maze2d-umaze-dense-v0', 'maze2d-medium-dense-v0', 'maze2d-large-dense-v0', 'maze2d-umaze-dense-v1', 'maze2d-medium-dense-v1', 'maze2d-large-dense-v1', 'maze2d-eval-umaze-dense-v1', 'maze2d-eval-medium-dense-v1', 'maze2d-eval-large-dense-v1', 'minigrid-fourrooms-v0', 'minigrid-fourrooms-random-v0', 'hopper-random-v1', 'hopper-random-v2', 'hopper-medium-v1', 'hopper-medium-v2', 'hopper-expert-v1', 'hopper-expert-v2', 'hopper-medium-expert-v1', 'hopper-medium-expert-v2', 'hopper-medium-replay-v1', 'hopper-medium-replay-v2', 'hopper-full-replay-v1', 'hopper-full-replay-v2', 'halfcheetah-random-v1', 'halfcheetah-random-v2', 'halfcheetah-medium-v1', 'halfcheetah-medium-v2', 'halfcheetah-expert-v1', 'halfcheetah-expert-v2', 'halfcheetah-medium-expert-v1', 'halfcheetah-medium-expert-v2', 'halfcheetah-medium-replay-v1', 'halfcheetah-medium-replay-v2', 'halfcheetah-full-replay-v1', 'halfcheetah-full-replay-v2', 'ant-random-v1', 'ant-random-v2', 'ant-medium-v1', 'ant-medium-v2', 'ant-expert-v1', 'ant-expert-v2', 'ant-medium-expert-v1', 'ant-medium-expert-v2', 'ant-medium-replay-v1', 'ant-medium-replay-v2', 'ant-full-replay-v1', 'ant-full-replay-v2', 'walker2d-random-v1', 'walker2d-random-v2', 'walker2d-medium-v1', 'walker2d-medium-v2', 'walker2d-expert-v1', 'walker2d-expert-v2', 'walker2d-medium-expert-v1', 'walker2d-medium-expert-v2', 'walker2d-medium-replay-v1', 'walker2d-medium-replay-v2', 'walker2d-full-replay-v1', 'walker2d-full-replay-v2', 'hopper-medium-v0', 'halfcheetah-medium-v0', 'walker2d-medium-v0', 'hopper-expert-v0', 'halfcheetah-expert-v0', 'walker2d-expert-v0', 'hopper-random-v0', 'halfcheetah-random-v0', 'walker2d-random-v0', 'hopper-medium-replay-v0', 'walker2d-medium-replay-v0', 'halfcheetah-medium-replay-v0', 'walker2d-medium-expert-v0', 'halfcheetah-medium-expert-v0', 'hopper-medium-expert-v0', 'ant-medium-expert-v0', 'ant-medium-replay-v0', 'ant-medium-v0', 'ant-random-v0', 'ant-expert-v0', 'ant-random-expert-v0', 'kitchen_relax-v1', 'kitchen-complete-v0', 'kitchen-partial-v0', 'kitchen-mixed-v0', 'HumanoidDeepMimicBackflipBulletEnv-v1', 'HumanoidDeepMimicWalkBulletEnv-v1', 'CartPoleBulletEnv-v1', 'CartPoleContinuousBulletEnv-v0', 'MinitaurBulletEnv-v0', 'MinitaurBulletDuckEnv-v0', 'MinitaurExtendedEnv-v0', 'MinitaurReactiveEnv-v0', 'MinitaurBallGymEnv-v0', 'MinitaurTrottingEnv-v0', 'MinitaurStandGymEnv-v0', 'MinitaurAlternatingLegsEnv-v0', 'MinitaurFourLegStandEnv-v0', 'RacecarBulletEnv-v0', 'RacecarZedBulletEnv-v0', 'KukaBulletEnv-v0', 'KukaCamBulletEnv-v0', 'KukaDiverseObjectGrasping-v0', 'InvertedPendulumBulletEnv-v0', 'InvertedDoublePendulumBulletEnv-v0', 'InvertedPendulumSwingupBulletEnv-v0', 'ReacherBulletEnv-v0', 'PusherBulletEnv-v0', 'ThrowerBulletEnv-v0', 'Walker2DBulletEnv-v0', 'HalfCheetahBulletEnv-v0', 'AntBulletEnv-v0', 'HopperBulletEnv-v0', 'HumanoidBulletEnv-v0', 'HumanoidFlagrunBulletEnv-v0', 'HumanoidFlagrunHarderBulletEnv-v0']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## download the dataset for the hopper environment","metadata":{}},{"cell_type":"code","source":"\n# Create the environment\nenv = gym.make('hopper-expert-v2') #gym.make('maze2d-umaze-v1')\n\nprint(\"reset\",env.reset())\nprint(\"step\",env.step(env.action_space.sample()))\n\n#get the dataset for hopper\ndataset = env.get_dataset()\ndataset.keys()","metadata":{"execution":{"iopub.status.busy":"2024-08-14T19:37:11.771545Z","iopub.execute_input":"2024-08-14T19:37:11.771835Z","iopub.status.idle":"2024-08-14T19:37:13.186970Z","shell.execute_reply.started":"2024-08-14T19:37:11.771802Z","shell.execute_reply":"2024-08-14T19:37:13.186181Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/gym/spaces/box.py:74: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n  \"Box bound precision lowered by casting to {}\".format(self.dtype)\n","output_type":"stream"},{"name":"stdout","text":"reset [ 1.25353429e+00 -4.98018573e-03 -2.06140013e-03  7.10537286e-04\n -8.51438047e-04  2.61325936e-03  1.46858319e-03 -4.71780908e-03\n -6.77464173e-05 -3.80985516e-03  6.49444513e-05]\nstep (array([ 1.25329666e+00, -3.78357411e-03, -5.24973194e-04,  7.48895489e-04,\n       -4.18817085e-03,  8.41301716e-03, -6.09106548e-02,  3.02544383e-01,\n        3.83899592e-01,  1.10849440e-02, -8.33131008e-01]), 1.0050605256485636, False, {})\n","output_type":"stream"},{"name":"stderr","text":"load datafile: 100%|██████████| 21/21 [00:01<00:00, 15.31it/s]\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"dict_keys(['actions', 'infos/action_log_probs', 'infos/qpos', 'infos/qvel', 'metadata/algorithm', 'metadata/iteration', 'metadata/policy/fc0/bias', 'metadata/policy/fc0/weight', 'metadata/policy/fc1/bias', 'metadata/policy/fc1/weight', 'metadata/policy/last_fc/bias', 'metadata/policy/last_fc/weight', 'metadata/policy/last_fc_log_std/bias', 'metadata/policy/last_fc_log_std/weight', 'metadata/policy/nonlinearity', 'metadata/policy/output_distribution', 'next_observations', 'observations', 'rewards', 'terminals', 'timeouts'])"},"metadata":{}}]},{"cell_type":"code","source":" next_state, reward, done, info=env.step(env.action_space.sample())\nprint(next_state.shape)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-14T19:37:14.794167Z","iopub.execute_input":"2024-08-14T19:37:14.794970Z","iopub.status.idle":"2024-08-14T19:37:14.802261Z","shell.execute_reply.started":"2024-08-14T19:37:14.794925Z","shell.execute_reply":"2024-08-14T19:37:14.801483Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"(11,)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## let's check the dataset","metadata":{}},{"cell_type":"code","source":"print(env.observation_space.shape)\n#the observations from the hopper-website\nobsers= [\"torso_z\", \"torso_angle\", \"thigh_angle\", \"leg_angle\", \"foot_angle\", \"velocity_x\", \"velocity_z\", \"torso_ang_velocity\", \n         \"thigh_ang_velocity\", \"torso_ang_velocity\", \"foot_ang_velocity\"]\nprint(len(obsers))\n#print the dataset attributes\ndataset.keys()","metadata":{"execution":{"iopub.status.busy":"2024-08-14T19:37:16.690547Z","iopub.execute_input":"2024-08-14T19:37:16.690844Z","iopub.status.idle":"2024-08-14T19:37:16.698788Z","shell.execute_reply.started":"2024-08-14T19:37:16.690807Z","shell.execute_reply":"2024-08-14T19:37:16.697861Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"(11,)\n11\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"dict_keys(['actions', 'infos/action_log_probs', 'infos/qpos', 'infos/qvel', 'metadata/algorithm', 'metadata/iteration', 'metadata/policy/fc0/bias', 'metadata/policy/fc0/weight', 'metadata/policy/fc1/bias', 'metadata/policy/fc1/weight', 'metadata/policy/last_fc/bias', 'metadata/policy/last_fc/weight', 'metadata/policy/last_fc_log_std/bias', 'metadata/policy/last_fc_log_std/weight', 'metadata/policy/nonlinearity', 'metadata/policy/output_distribution', 'next_observations', 'observations', 'rewards', 'terminals', 'timeouts'])"},"metadata":{}}]},{"cell_type":"markdown","source":"### attributes of the dataset\n* actions: Actions taken by the agent.\n* infos/action_log_probs: Log probabilities of the taken actions.\n* infos/qpos: Generalized positions of the Hopper's joints and body parts.\n* infos/qvel: Generalized velocities of the Hopper's joints and body parts.\n* metadata/algorithm: The algorithm used to generate expert data.\n* metadata/iteration: Iteration number when the data was collected.\n* metadata/policy/fc0/bias: Bias of the first fully connected layer in the policy network.\n* metadata/policy/fc0/weight: Weights of the first fully connected layer in the policy network.\n* metadata/policy/fc1/bias: Bias of the second fully connected layer in the policy network.\n* metadata/policy/fc1/weight: Weights of the second fully connected layer in the policy network.\n* metadata/policy/last_fc/bias: Bias of the last fully connected layer in the policy network.\n* metadata/policy/last_fc/weight: Weights of the last fully connected layer in the policy network.\n* metadata/policy/last_fc_log_std/bias: Bias for the log standard deviation in the last layer.\n* metadata/policy/last_fc_log_std/weight: Weights for the log standard deviation in the last layer.\n* metadata/policy/nonlinearity: Type of nonlinearity used in the policy network.\n* metadata/policy/output_distribution: Distribution type for the policy's output.\n* next_observations: Observations after the agent takes an action.\n* observations: Observations before the agent takes an action.\n* rewards: Rewards received after actions are taken.\n* terminals: Indicates if an episode ended (True/False).\n* timeouts: Indicates if an episode ended due to a time limit (True/False).\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\n#unpack the dataset\nactions= dataset['actions'] # Actions taken by the agent\nobservations= dataset['observations'] # The observations or states seen by the agent.\nrewards= dataset['rewards'] #The reward received after each action. .reshape(-1, 1)\nnext_observations= dataset['next_observations'] #next observations after the agent takes an action\nterminals= dataset['terminals'] #Indicates if the episode ended in an unhealty way (1 if ended, 0 otherwise) \ntimeouts= dataset ['timeouts'] #Indicates if the episode ended due to a timeout.\ndones= np.logical_or(terminals, timeouts)\n\nprint(\"shapes of our columns/attributes: actions=\", actions.shape, \" observations=\", observations.shape, \" rewards=\", rewards.shape)\nprint(\"next_observations=\", next_observations.shape, \" terminals=\", terminals.shape,\" timeouts=\", timeouts.shape, \"done=\", dones.shape )","metadata":{"execution":{"iopub.status.busy":"2024-08-14T19:37:17.744907Z","iopub.execute_input":"2024-08-14T19:37:17.745213Z","iopub.status.idle":"2024-08-14T19:37:17.755346Z","shell.execute_reply.started":"2024-08-14T19:37:17.745173Z","shell.execute_reply":"2024-08-14T19:37:17.754391Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"shapes of our columns/attributes: actions= (1000000, 3)  observations= (1000000, 11)  rewards= (1000000,)\nnext_observations= (1000000, 11)  terminals= (1000000,)  timeouts= (1000000,) done= (1000000,)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"split dataset into train and test ","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nimport numpy as np\nimport torch\n\ntrain_observations, test_observations, train_actions, test_actions, train_rewards, test_rewards, train_next_observations, test_next_observations,train_dones, test_dones = train_test_split(\n    observations, actions, rewards, next_observations, dones, test_size=0.2, random_state=42\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-14T19:37:18.717933Z","iopub.execute_input":"2024-08-14T19:37:18.718740Z","iopub.status.idle":"2024-08-14T19:37:19.863576Z","shell.execute_reply.started":"2024-08-14T19:37:18.718698Z","shell.execute_reply":"2024-08-14T19:37:19.862652Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass CustomDataset(Dataset):\n    def __init__(self, observations, actions, next_observations, rewards, dones):\n        super().__init__()\n        # Extract data\n        self.observations = torch.tensor(observations, dtype=torch.float32)\n        self.actions = torch.tensor(actions, dtype=torch.float32)\n        self.rewards = torch.tensor(rewards, dtype=torch.float32)\n        self.next_observations = torch.tensor(next_observations, dtype=torch.float32)\n        self.dones= torch.tensor(dones,dtype= torch.bool )\n        #self.timeouts= torch.tensor(timeouts, dtype= torch.bool)\n        \n\n    def __len__(self):\n        return len(self.observations)\n\n    def __getitem__(self, idx):\n        return self.observations[idx], self.actions[idx], self.next_observations[idx], self.rewards[idx], self.dones[idx]\n\nwhole_dataset= CustomDataset( observations, actions, rewards, next_observations,  dones)\n\ntrain_set = CustomDataset(train_observations, train_actions, train_next_observations, train_rewards, train_dones)\ntest_set = CustomDataset(test_observations, test_actions, test_next_observations, test_rewards, test_dones)\nprint(len(train_set),len(test_set) , len(whole_dataset))","metadata":{"execution":{"iopub.status.busy":"2024-08-14T19:37:20.869774Z","iopub.execute_input":"2024-08-14T19:37:20.870572Z","iopub.status.idle":"2024-08-14T19:37:20.961088Z","shell.execute_reply.started":"2024-08-14T19:37:20.870532Z","shell.execute_reply":"2024-08-14T19:37:20.960183Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"800000 200000 1000000\n","output_type":"stream"}]},{"cell_type":"markdown","source":"create dataloaders","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nbatch_size = 120\ntrain_loader = DataLoader(train_set, batch_size, shuffle=True)\ntest_loader = DataLoader(test_set, batch_size, shuffle=False)\ndata_loader= DataLoader(whole_dataset, batch_size, shuffle= True)","metadata":{"execution":{"iopub.status.busy":"2024-08-14T19:37:22.069750Z","iopub.execute_input":"2024-08-14T19:37:22.070434Z","iopub.status.idle":"2024-08-14T19:37:22.075726Z","shell.execute_reply.started":"2024-08-14T19:37:22.070398Z","shell.execute_reply":"2024-08-14T19:37:22.074977Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# create the agent","metadata":{}},{"cell_type":"markdown","source":"### The actor responsible for choosing an action given a specific state \"policy network\"","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.distributions import Normal\n\nclass Actor(nn.Module):\n    def __init__(self, state_size, action_size, hidden_size= 32, standard_deviation_min= -10, standard_deviation_max= 10):\n        \"\"\"\n        state_size: (input size).\n        action_size: (output size).\n        hidden_size: Number of neurons in the hidden layers (default 32).\n        standard_deviation_min and standard_deviation_max: values used to clamp the log standard deviation of the policy's action distribution to prevent it from becoming too high or too low.\n        \"\"\"\n        super(Actor, self).__init__()\n        self.standard_deviation_min= standard_deviation_min\n        self.standard_deviation_max= standard_deviation_max\n        self.lin1= nn.Linear(state_size, hidden_size)\n        self.lin2= nn.Linear(hidden_size, hidden_size)\n        #output Linear layers to get the mean (mu) and the log standard deviation (log_sigma) of the action distribution.\n        self.mu= nn.Linear(hidden_size, action_size) # mean\n        self.log_sigma= nn.Linear(hidden_size, action_size) #standard deviation\n    \n    def forward(self, state):\n        x= F.relu(self.lin2(F.relu(self.lin1(state))))\n        \n        mu = torch.tanh(self.mu(x)) #The mean is passed through a tanh activation to keep the action within a certain range.\n        \n        log_sigma= self.log_sigma(x)\n        log_sigma = torch.clamp(log_sigma, self.standard_deviation_min, self.standard_deviation_max)\n        return mu, log_sigma\n    \n    def evaluate(self, state):\n        mu, log_sigma= self.forward(state) \n        sigma= log_sigma.exp()\n        normal_distribution= Normal(mu, sigma) #create normal distriution\n        \n        action= normal_distribution.sample() #get an action from the distriution\n        return action, normal_distribution \n    \n    def get_action(self, state): #to be used in the evaluation phase\n        mu, log_sigma= self.forward(state) \n        sigma= log_sigma.exp()\n        normal_distribution= Normal(mu, sigma) #create normal distriution\n        \n        action= normal_distribution.sample() #get an action from the distriution\n        return action.detach().cpu()\n    \n    def get_deterministic_action(self, state):\n        mu, _= self.forward(state)\n        return mu.detach().cpu()\n        \n        \n        \n        ","metadata":{"execution":{"iopub.status.busy":"2024-08-14T19:37:23.689094Z","iopub.execute_input":"2024-08-14T19:37:23.689650Z","iopub.status.idle":"2024-08-14T19:37:23.703478Z","shell.execute_reply.started":"2024-08-14T19:37:23.689609Z","shell.execute_reply":"2024-08-14T19:37:23.702371Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"### the critic estimates the Q-value, which is the expected return for taking a particular action in a given state.","metadata":{}},{"cell_type":"code","source":"class Critic(nn.Module):\n    def __init__(self, state_size, action_size, hidden_size= 32,seed= 1):\n        super(Critic, self).__init__()\n        torch.manual_seed(1)\n        self.lin1= nn.Linear(state_size+action_size, hidden_size)\n        self.lin2 = nn.Linear(hidden_size, hidden_size)\n        self.lin3= nn.Linear(hidden_size, 1)\n        \n    def forward(self, state, action):\n        #print(\"Critic\", state.shape, action.shape)\n        x= torch.cat((state, action), dim=-1)\n        #print(x.shape)\n        x= F.relu(self.lin2(F.relu(self.lin1(x))))\n        return self.lin3(x)","metadata":{"execution":{"iopub.status.busy":"2024-08-14T19:37:24.744572Z","iopub.execute_input":"2024-08-14T19:37:24.744874Z","iopub.status.idle":"2024-08-14T19:37:24.752184Z","shell.execute_reply.started":"2024-08-14T19:37:24.744840Z","shell.execute_reply":"2024-08-14T19:37:24.751410Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"###  the value estimates the expected return from a state, independent of any specific action.","metadata":{}},{"cell_type":"code","source":"class Value(nn.Module):\n    def __init__(self, state_size, hidden_size= 32):\n        super(Value, self).__init__()\n        self.lin1= nn.Linear(state_size, hidden_size)\n        self.lin2= nn.Linear(hidden_size, hidden_size)\n        self.lin3= nn.Linear(hidden_size, 1)\n        \n    def forward(self, state):\n        x= F.relu(self.lin2(F.relu(self.lin1(state))))\n        return self.lin3(x)","metadata":{"execution":{"iopub.status.busy":"2024-08-14T19:37:25.886994Z","iopub.execute_input":"2024-08-14T19:37:25.887296Z","iopub.status.idle":"2024-08-14T19:37:25.895521Z","shell.execute_reply.started":"2024-08-14T19:37:25.887258Z","shell.execute_reply":"2024-08-14T19:37:25.894565Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"### create the replay buffer","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport random\nimport torch\nfrom collections import deque, namedtuple\n\nclass ReplayBuffer:\n    def __init__(self, max_size, batch_size, device):\n        self.device= device\n        self.memory= deque(maxlen= max_size)\n        self.batch_size= batch_size\n        #define expericences tuple\n        self.experience= namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n        \n    def add(self, state, action, reward, next_state, done):\n        e= self.experience(state, action, reward, next_state, done)\n        self.memory.append(e)\n        \n    def sample(self):\n        #sample randome experiences from memory to train the model i=on it\n        experiences = random.sample(self.memory, k=self.batch_size)\n        #unpacked them into seperate tensors\n        states = torch.from_numpy(np.stack([e.state for e in experiences if e is not None])).float().to(self.device)\n        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(self.device)\n        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(self.device)\n        next_states = torch.from_numpy(np.stack([e.next_state for e in experiences if e is not None])).float().to(self.device)\n        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(self.device)\n        return (states, actions, rewards, next_states, terminals,timeouts )\n\n    def __len__(self):\n        return len(self.memory)","metadata":{"execution":{"iopub.status.busy":"2024-08-14T19:37:26.849441Z","iopub.execute_input":"2024-08-14T19:37:26.849772Z","iopub.status.idle":"2024-08-14T19:37:26.863916Z","shell.execute_reply.started":"2024-08-14T19:37:26.849734Z","shell.execute_reply":"2024-08-14T19:37:26.863039Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.optim as optim\nimport torch.nn as nn\nfrom torch.nn.utils import clip_grad_norm_\n\nclass Implicit_Q_learning(nn.Module):\n    def __init__(self, state_size, action_size, lr, hidden_size, tau, temperature, expectile, device):\n        super(Implicit_Q_learning, self).__init__()\n        self.state_size= state_size\n        self.action_size= action_size\n        self.lr= lr\n        self.hidden_size= hidden_size\n        self.tau= tau #update parameter for the target netwrok\n        self.device = device\n        self.temperature= torch.FloatTensor([temperature]).to(self.device)\n        self.expectile= torch.FloatTensor([expectile]).to(self.device)\n        self.gamma = torch.FloatTensor([0.99]).to(device) #to compute the Q-values\n        self.clip_grad_param = 1\n\n        #____define the neuralnetwroks____\n        # Actor_network: outputs actions based on the state.\n        self.actor_local= Actor(self.state_size, self.action_size, self.hidden_size).to(self.device)\n        self.actor_optimizer= optim.Adam(self.actor_local.parameters(), lr= self.lr)\n\n        # Critic_networks: Two Q-value networks to estimate the value of state-action pairs.\n        self.critic1= Critic(self.state_size, self.action_size, self.hidden_size, seed= 2).to(self.device)\n        self.critic2= Critic(self.state_size, self.action_size, self.hidden_size, seed= 1).to(self.device)\n        assert self.critic1.parameters() != self.critic2.parameters()\n\n        self.critic1_optimizer = optim.Adam(self.critic1.parameters(), lr=self.lr)\n        self.critic2_optimizer = optim.Adam(self.critic2.parameters(), lr=self.lr) \n\n        #Target networks for stability, which slowly track the critic networks.\n        self.critic1_target = Critic(self.state_size, self.action_size, self.hidden_size).to(self.device)\n        self.critic1_target.load_state_dict(self.critic1.state_dict())\n\n        self.critic2_target = Critic(self.state_size, self.action_size, self.hidden_size).to(self.device)\n        self.critic2_target.load_state_dict(self.critic2.state_dict())\n\n        #Value_network: estimates the value of a state independently of the action.\n        self.value_net = Value(self.state_size, self.hidden_size).to(self.device)\n        self.value_optimizer = optim.Adam(self.value_net.parameters(), lr=self.lr)\n        \n    def get_action(self, state, eval= False):\n        state= torch.from_numpy(state).float().to(self.device)\n        with torch.no_grad():\n            if eval:\n                action = self.actor_local.get_deterministic_action(state)\n            else:\n                action = self.actor_local.get_action(state)\n        return action.numpy()\n    \n    def calc_policy_loss(self, states, actions):\n        with torch.no_grad():\n            v = self.value_net(states)\n            q1 = self.critic1_target(states, actions)\n            q2 = self.critic2_target(states, actions)\n            min_Q = torch.min(q1,q2)\n\n        exp_a = torch.exp((min_Q - v) * self.temperature)\n        exp_a = torch.min(exp_a, torch.FloatTensor([100.0]).to(states.device))\n\n        _, dist = self.actor_local.evaluate(states)\n        log_probs = dist.log_prob(actions)\n        actor_loss = -(exp_a * log_probs).mean()\n\n        return actor_loss\n    \n    def calc_value_loss(self, states, actions):\n        with torch.no_grad():\n            q1 = self.critic1_target(states, actions)   \n            q2 = self.critic2_target(states, actions)\n            min_Q = torch.min(q1,q2)\n        \n        value = self.value_net(states)\n        value_loss = loss(min_Q - value, self.expectile).mean()\n        return value_loss\n    \n    def calc_q_loss(self, states, actions, rewards, dones, next_states):\n        with torch.no_grad():\n            next_v = self.value_net(next_states)\n            dones = dones.float()\n            q_target = rewards + (self.gamma * (1 - dones) * next_v) \n            #q_target = rewards + (self.gamma * (1 - terminals) * (1 - timeouts) * next_v)\n\n\n        q1 = self.critic1(states, actions)\n        q2 = self.critic2(states, actions)\n        critic1_loss = ((q1 - q_target)**2).mean() \n        critic2_loss = ((q2 - q_target)**2).mean()\n        return critic1_loss, critic2_loss\n\n\n    def learn(self, experiences):\n        states, actions, rewards, next_states,  dones  = experiences\n\n        self.value_optimizer.zero_grad()\n        #print(states, actions)\n        #print(states.shape, actions.shape)\n        value_loss = self.calc_value_loss(states, actions)\n        value_loss.backward()\n        self.value_optimizer.step()\n\n        actor_loss = self.calc_policy_loss(states, actions)\n        self.actor_optimizer.zero_grad()\n        actor_loss.backward()\n        self.actor_optimizer.step()\n        \n        critic1_loss, critic2_loss = self.calc_q_loss(states, actions, rewards, dones, next_states)\n\n        # critic 1\n        self.critic1_optimizer.zero_grad()\n        critic1_loss.backward()\n        clip_grad_norm_(self.critic1.parameters(), self.clip_grad_param)\n        self.critic1_optimizer.step()\n        # critic 2\n        self.critic2_optimizer.zero_grad()\n        critic2_loss.backward()\n        clip_grad_norm_(self.critic2.parameters(), self.clip_grad_param)\n        self.critic2_optimizer.step()\n\n        # ----------------------- update target networks ----------------------- #\n        self.soft_update(self.critic1, self.critic1_target)\n        self.soft_update(self.critic2, self.critic2_target)\n        \n        return actor_loss.item(), critic1_loss.item(), critic2_loss.item(), value_loss.item()\n\n    def soft_update(self, local_model , target_model):\n        \"\"\"Soft update model parameters.\n        θ_target = τ*θ_local + (1 - τ)*θ_target\n        Params\n        ======\n            local_model: PyTorch model (weights will be copied from)\n            target_model: PyTorch model (weights will be copied to)\n            tau (float): interpolation parameter \n        \"\"\"\n        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n            target_param.data.copy_(self.tau*local_param.data + (1.0-self.tau)*target_param.data)\n\n\n            \n            \n            \n            ","metadata":{"execution":{"iopub.status.busy":"2024-08-14T19:37:27.413974Z","iopub.execute_input":"2024-08-14T19:37:27.414261Z","iopub.status.idle":"2024-08-14T19:37:27.448705Z","shell.execute_reply.started":"2024-08-14T19:37:27.414227Z","shell.execute_reply":"2024-08-14T19:37:27.447772Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"#hyperparameters\nimport torch\nimport numpy as np\nimport random\n\n\nrun_name= \"IQL\"\nenv_name= \"hopper-expert-v2\" # environment name\nepisodes= 100 #number of episodes\nseed= 1 \nlog_video= 0  #\"Log agent behaviour to wanbd when set to 1, default: 0\"\nsave_every= 100 #save the network every x epochs\nbatch_size = 120\nhidden_size= 256\nlearning_rate= 3e-4\ntemperature= 3\nexpectile= 0.7\ntau= 5e-3\neval_every= 1\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n#define the train loader\ndataloader= DataLoader(whole_dataset, batch_size, shuffle= True)\n\n#the training loop\n#set the seeds\nnp.random.seed(seed)\nrandom.seed(seed)\ntorch.manual_seed(seed)\n#define an environment\nenv = gym.make('hopper-expert-v2') \n#set the seed for the action space\nenv.action_space.seed(seed)\n\nbatches = 0\naverage10 = deque(maxlen=10)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-14T19:37:28.247000Z","iopub.execute_input":"2024-08-14T19:37:28.247291Z","iopub.status.idle":"2024-08-14T19:37:28.316301Z","shell.execute_reply.started":"2024-08-14T19:37:28.247257Z","shell.execute_reply":"2024-08-14T19:37:28.315394Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/gym/spaces/box.py:74: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n  \"Box bound precision lowered by casting to {}\".format(self.dtype)\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport os\nimport wandb\nimport random\n\ndef save(args, save_name, model, ep=None):\n    save_dir = './trained_models/' \n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir)\n    \n    save_path = os.path.join(save_dir, run_name + save_name)\n    if ep is not None:\n        save_path += str(ep) + \".pth\"\n    else:\n        save_path += \".pth\"\n    \n    torch.save(model.state_dict(), save_path)\n    wandb.save(save_path)\n\ndef collect_random(env, dataset, num_samples=200):\n    state = env.reset()\n    for _ in range(num_samples):\n        action = env.action_space.sample()\n        next_state, reward, termi, _ = env.step(action)\n        dataset.add(state, action, reward, next_state, done)\n        state = next_state\n        if terminals or timeout:\n            state = env.reset()\n\ndef loss(diff, expectile=0.8):\n    weight = torch.where(diff > 0, expectile, (1 - expectile))\n    return weight * (diff**2)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-14T19:37:30.911242Z","iopub.execute_input":"2024-08-14T19:37:30.911524Z","iopub.status.idle":"2024-08-14T19:37:31.224340Z","shell.execute_reply.started":"2024-08-14T19:37:30.911477Z","shell.execute_reply":"2024-08-14T19:37:31.223414Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"'\\n\\n# Simulate training\\nepochs = 10\\noffset = random.random() / 5\\nfor epoch in range(2, epochs):\\n    acc = 1 - 2 ** -epoch - random.random() / epoch - offset\\n    loss_value = 2 ** -epoch + random.random() / epoch + offset\\n\\n    # Log metrics to WandB\\n    if wandb:\\n        wandb.log({\"acc\": acc, \"loss\": loss_value})\\n\\n# [optional] finish the WandB run, necessary in notebooks\\nif wandb:\\n    wandb.finish()\\n'"},"metadata":{}}]},{"cell_type":"code","source":"def evaluate(env, policy, eval_runs=5): \n    \"\"\"\n    Makes an evaluation run with the current policy\n    \"\"\"\n    reward_batch = []\n    for i in range(eval_runs):\n        state = env.reset()\n\n        rewards = 0\n        while True:\n            action = policy.get_action(state, eval=True)\n\n            state, reward, done, _ = env.step(action)\n            rewards += reward\n            if done:\n                break\n        reward_batch.append(rewards)\n    return np.mean(reward_batch)","metadata":{"execution":{"iopub.status.busy":"2024-08-14T19:37:31.625632Z","iopub.execute_input":"2024-08-14T19:37:31.625936Z","iopub.status.idle":"2024-08-14T19:37:31.632458Z","shell.execute_reply.started":"2024-08-14T19:37:31.625903Z","shell.execute_reply":"2024-08-14T19:37:31.631678Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"! pip install protobuf==3.20.0","metadata":{"execution":{"iopub.status.busy":"2024-08-14T19:37:32.785372Z","iopub.execute_input":"2024-08-14T19:37:32.785690Z","iopub.status.idle":"2024-08-14T19:37:41.897793Z","shell.execute_reply.started":"2024-08-14T19:37:32.785656Z","shell.execute_reply":"2024-08-14T19:37:41.896805Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Requirement already satisfied: protobuf==3.20.0 in /opt/conda/lib/python3.7/site-packages (3.20.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install wandb","metadata":{"execution":{"iopub.status.busy":"2024-08-14T19:37:41.899944Z","iopub.execute_input":"2024-08-14T19:37:41.900198Z","iopub.status.idle":"2024-08-14T19:37:51.009669Z","shell.execute_reply.started":"2024-08-14T19:37:41.900169Z","shell.execute_reply":"2024-08-14T19:37:51.008671Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Requirement already satisfied: wandb in /opt/conda/lib/python3.7/site-packages (0.12.7)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: protobuf>=3.12.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (3.20.0)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (5.8.0)\nRequirement already satisfied: Click!=8.0.0,>=7.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (8.0.3)\nRequirement already satisfied: six>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (1.16.0)\nRequirement already satisfied: pathtools in /opt/conda/lib/python3.7/site-packages (from wandb) (0.1.2)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (2.25.1)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.7/site-packages (from wandb) (6.0)\nRequirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.7/site-packages (from wandb) (2.8.0)\nRequirement already satisfied: GitPython>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (3.1.24)\nRequirement already satisfied: shortuuid>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (1.0.8)\nRequirement already satisfied: yaspin>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (2.1.0)\nRequirement already satisfied: promise<3,>=2.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (2.3)\nRequirement already satisfied: subprocess32>=3.5.3 in /opt/conda/lib/python3.7/site-packages (from wandb) (3.5.4)\nRequirement already satisfied: configparser>=3.8.1 in /opt/conda/lib/python3.7/site-packages (from wandb) (5.1.0)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (1.5.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from Click!=8.0.0,>=7.0->wandb) (4.8.2)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.7/site-packages (from GitPython>=1.0.0->wandb) (4.0.9)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from GitPython>=1.0.0->wandb) (3.10.0.2)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (4.0.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (1.26.7)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (2.10)\nRequirement already satisfied: termcolor<2.0.0,>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from yaspin>=1.0.0->wandb) (1.1.0)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (3.0.5)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->Click!=8.0.0,>=7.0->wandb) (3.6.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"import wandb\nwandb.login(key= \"512e30c6b6b9ff00a20d3fec64a6cab8320827df\")","metadata":{"execution":{"iopub.status.busy":"2024-08-15T09:07:49.616245Z","iopub.execute_input":"2024-08-15T09:07:49.616600Z","iopub.status.idle":"2024-08-15T09:07:51.435061Z","shell.execute_reply.started":"2024-08-15T09:07:49.616507Z","shell.execute_reply":"2024-08-15T09:07:51.434339Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured (use `wandb login --relogin` to force relogin)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publically.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"\"\"\"# Start a W&B Run with wandb.init\nrun = wandb.init(project=\"offline-RL\", config={\"architecture\": \"IQL\",\"dataset\": \"hopper-expert-v2\"})\nif run:\n    print(\"state_size= \", env.observation_space.shape[0])\n    print(\"action_size= \", env.action_space.shape[0])\n    agent = Implicit_Q_learning(state_size=env.observation_space.shape[0],\n            action_size=env.action_space.shape[0],\n            lr=learning_rate,\n            hidden_size=hidden_size,\n            tau=tau,\n            temperature=temperature,\n            expectile=expectile,\n            device=device)\n    run.watch(agent, log=\"gradients\", log_freq=10)\n    if log_video:\n        env = gym.wrappers.Monitor(env, './video', video_callable=lambda x: x%10==0, force=True)\n    eval_reward = evaluate(env, agent)\n    run.log({\"Test Reward\": eval_reward, \"Episode\": 0, \"Batches\": batches}, step=batches)\n    for i in range(1, episodes+1):\n        for batch_idx, experience in enumerate(dataloader):\n            states, actions, rewards, next_states, dones = experience\n            states = states.to(device)\n            actions = actions.to(device)\n            rewards = rewards.to(device)\n            next_states = next_states.to(device)\n            dones = dones.to(device)\n            policy_loss, critic1_loss, critic2_loss, value_loss = agent.learn((states, actions, rewards, next_states, dones))\n            batches += 1\n            \n        if i % eval_every == 0:\n            eval_reward = evaluate(env, agent)\n            run.log({\"Test Reward\": eval_reward, \"Episode\": i, \"Batches\": batches}, step=batches)\n\n            average10.append(eval_reward)\n            print(\"Episode: {} | Reward: {} | Polciy Loss: {} | Batches: {}\".format(i, eval_reward, policy_loss, batches))\n            \n        run.log({\n            \"Average10\": np.mean(average10),\n            \"Policy Loss\": policy_loss,\n            \"Value Loss\": value_loss,\n            \"Critic 1 Loss\": critic1_loss,\n            \"Critic 2 Loss\": critic2_loss,\n            \"Batches\": batches,\n            \"Episode\": i})\n        if (i %10 == 0) and log_video:\n            mp4list = glob.glob('video/*.mp4')\n            if len(mp4list) > 1:\n                mp4 = mp4list[-2]\n                run.log({\"gameplays\": run.Video(mp4, caption='episode: '+str(i-10), fps=4, format=\"gif\"), \"Episode\": i})\n\n  \n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-08-14T19:38:38.291686Z","iopub.execute_input":"2024-08-14T19:38:38.292003Z","iopub.status.idle":"2024-08-14T19:38:38.300952Z","shell.execute_reply.started":"2024-08-14T19:38:38.291966Z","shell.execute_reply":"2024-08-14T19:38:38.300082Z"},"jupyter":{"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"'# Start a W&B Run with wandb.init\\nrun = wandb.init(project=\"offline-RL\", config={\"architecture\": \"IQL\",\"dataset\": \"hopper-expert-v2\"})\\nif run:\\n    print(\"state_size= \", env.observation_space.shape[0])\\n    print(\"action_size= \", env.action_space.shape[0])\\n    agent = Implicit_Q_learning(state_size=env.observation_space.shape[0],\\n            action_size=env.action_space.shape[0],\\n            lr=learning_rate,\\n            hidden_size=hidden_size,\\n            tau=tau,\\n            temperature=temperature,\\n            expectile=expectile,\\n            device=device)\\n    run.watch(agent, log=\"gradients\", log_freq=10)\\n    if log_video:\\n        env = gym.wrappers.Monitor(env, \\'./video\\', video_callable=lambda x: x%10==0, force=True)\\n    eval_reward = evaluate(env, agent)\\n    run.log({\"Test Reward\": eval_reward, \"Episode\": 0, \"Batches\": batches}, step=batches)\\n    for i in range(1, episodes+1):\\n        for batch_idx, experience in enumerate(dataloader):\\n            states, actions, rewards, next_states, dones = experience\\n            states = states.to(device)\\n            actions = actions.to(device)\\n            rewards = rewards.to(device)\\n            next_states = next_states.to(device)\\n            dones = dones.to(device)\\n            policy_loss, critic1_loss, critic2_loss, value_loss = agent.learn((states, actions, rewards, next_states, dones))\\n            batches += 1\\n            \\n        if i % eval_every == 0:\\n            eval_reward = evaluate(env, agent)\\n            run.log({\"Test Reward\": eval_reward, \"Episode\": i, \"Batches\": batches}, step=batches)\\n\\n            average10.append(eval_reward)\\n            print(\"Episode: {} | Reward: {} | Polciy Loss: {} | Batches: {}\".format(i, eval_reward, policy_loss, batches))\\n            \\n        run.log({\\n            \"Average10\": np.mean(average10),\\n            \"Policy Loss\": policy_loss,\\n            \"Value Loss\": value_loss,\\n            \"Critic 1 Loss\": critic1_loss,\\n            \"Critic 2 Loss\": critic2_loss,\\n            \"Batches\": batches,\\n            \"Episode\": i})\\n        if (i %10 == 0) and log_video:\\n            mp4list = glob.glob(\\'video/*.mp4\\')\\n            if len(mp4list) > 1:\\n                mp4 = mp4list[-2]\\n                run.log({\"gameplays\": run.Video(mp4, caption=\\'episode: \\'+str(i-10), fps=4, format=\"gif\"), \"Episode\": i})\\n\\n  \\n'"},"metadata":{}}]},{"cell_type":"code","source":"#add saving the bext models\n# Start a W&B Run with wandb.init\nrun = wandb.init(project=\"offline-RL\", config={\"architecture\": \"IQL\", \"dataset\": \"hopper-expert-v2\"})\nbest_eval_reward = -float(\"inf\")\nbest_model = {}\n\nif run:\n    print(\"state_size= \", env.observation_space.shape[0])\n    print(\"action_size= \", env.action_space.shape[0])\n    \n    agent = Implicit_Q_learning(state_size=env.observation_space.shape[0],\n                                action_size=env.action_space.shape[0],\n                                lr=learning_rate,\n                                hidden_size=hidden_size,\n                                tau=tau,\n                                temperature=temperature,\n                                expectile=expectile,\n                                device=device)\n                                \n    run.watch(agent, log=\"gradients\", log_freq=10)\n    \n    if log_video:\n        env = gym.wrappers.Monitor(env, './video', video_callable=lambda x: x % 10 == 0, force=True)\n    \n    eval_reward = evaluate(env, agent)\n    run.log({\"Test Reward\": eval_reward, \"Episode\": 0, \"Batches\": batches}, step=batches)\n    \n    for i in range(1, episodes + 1):\n        for batch_idx, experience in enumerate(dataloader):\n            states, actions, rewards, next_states, dones = experience\n            states = states.to(device)\n            actions = actions.to(device)\n            rewards = rewards.to(device)\n            next_states = next_states.to(device)\n            dones = dones.to(device)\n            \n            policy_loss, critic1_loss, critic2_loss, value_loss = agent.learn((states, actions, rewards, next_states, dones))\n            batches += 1\n            \n        if i % eval_every == 0:\n            eval_reward = evaluate(env, agent)\n            run.log({\"Test Reward\": eval_reward, \"Episode\": i, \"Batches\": batches}, step=batches)\n\n            average10.append(eval_reward)\n            print(\"Episode: {} | Reward: {} | Policy Loss: {} | Batches: {}\".format(i, eval_reward, policy_loss, batches))\n            \n            # Save the best model\n            if eval_reward > best_eval_reward:\n                best_eval_reward = eval_reward\n                best_model['actor_local'] = agent.actor_local.state_dict()\n                best_model['critic1'] = agent.critic1.state_dict()\n                best_model['critic2'] = agent.critic2.state_dict()\n                best_model['critic1_target'] = agent.critic1_target.state_dict()\n                best_model['critic2_target'] = agent.critic2_target.state_dict()\n                best_model['value'] = agent.value_net.state_dict()\n            \n        run.log({\n            \"Average10\": np.mean(average10),\n            \"Policy Loss\": policy_loss,\n            \"Value Loss\": value_loss,\n            \"Critic 1 Loss\": critic1_loss,\n            \"Critic 2 Loss\": critic2_loss,\n            \"Batches\": batches,\n            \"Episode\": i})\n        \n        if (i % 10 == 0) and log_video:\n            mp4list = glob.glob('video/*.mp4')\n            if len(mp4list) > 1:\n                mp4 = mp4list[-2]\n                run.log({\"gameplays\": run.Video(mp4, caption='episode: ' + str(i - 10), fps=4, format=\"gif\"), \"Episode\": i})\n    \n    # Save the best model at the end\n    torch.save(best_model, \"best_model.pth\")\n    run.log_artifact(\"best_model.pth\", type=\"model\")\n\n# End the run\nrun.finish()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-14T19:46:43.574671Z","iopub.execute_input":"2024-08-14T19:46:43.575019Z","iopub.status.idle":"2024-08-15T02:30:29.677889Z","shell.execute_reply.started":"2024-08-14T19:46:43.574964Z","shell.execute_reply":"2024-08-15T02:30:29.676604Z"},"trusted":true},"execution_count":31,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Finishing last run (ID:2ezhkhyw) before initializing another..."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br/>Waiting for W&B process to finish, PID 1727... <strong style=\"color:green\">(success).</strong>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\">\n<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Batches</td><td>▁█</td></tr><tr><td>Episode</td><td>▁█</td></tr><tr><td>Test Reward</td><td>▁█</td></tr></table><br/></div><div class=\"wandb-col\">\n<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Batches</td><td>16668</td></tr><tr><td>Episode</td><td>1</td></tr><tr><td>Test Reward</td><td>86.3021</td></tr></table>\n</div></div>\nSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n<br/>Synced <strong style=\"color:#cdcd00\">jolly-waterfall-27</strong>: <a href=\"https://wandb.ai/anwar96ibrahim-student/offline-RL/runs/2ezhkhyw\" target=\"_blank\">https://wandb.ai/anwar96ibrahim-student/offline-RL/runs/2ezhkhyw</a><br/>\nFind logs at: <code>./wandb/run-20240814_194317-2ezhkhyw/logs</code><br/>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Successfully finished last run (ID:2ezhkhyw). Initializing new run:<br/>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.6 is available!  To upgrade, please run:\n\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n                    Syncing run <strong><a href=\"https://wandb.ai/anwar96ibrahim-student/offline-RL/runs/2cyo6alt\" target=\"_blank\">polished-cherry-28</a></strong> to <a href=\"https://wandb.ai/anwar96ibrahim-student/offline-RL\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n\n                "},"metadata":{}},{"name":"stdout","text":"state_size=  11\naction_size=  3\nEpisode: 1 | Reward: 81.95221891158778 | Policy Loss: 0.05428459867835045 | Batches: 25002\nEpisode: 2 | Reward: 209.2821315378973 | Policy Loss: 0.022939754649996758 | Batches: 33336\nEpisode: 3 | Reward: 59.74656674180674 | Policy Loss: 0.11758668720722198 | Batches: 41670\nEpisode: 4 | Reward: 449.2906111652854 | Policy Loss: -0.03548737242817879 | Batches: 50004\nEpisode: 5 | Reward: 221.97566114436904 | Policy Loss: 0.4225108325481415 | Batches: 58338\nEpisode: 7 | Reward: 505.251053772187 | Policy Loss: -0.41293153166770935 | Batches: 75006\nEpisode: 8 | Reward: 441.288797620247 | Policy Loss: -0.02684936672449112 | Batches: 83340\nEpisode: 9 | Reward: 435.57832213471977 | Policy Loss: -0.7155187726020813 | Batches: 91674\nEpisode: 10 | Reward: 442.3668518670248 | Policy Loss: -0.3005243241786957 | Batches: 100008\nEpisode: 11 | Reward: 751.697295254217 | Policy Loss: -0.14835263788700104 | Batches: 108342\nEpisode: 12 | Reward: 448.14029274588245 | Policy Loss: 0.23266974091529846 | Batches: 116676\nEpisode: 13 | Reward: 739.4039735098199 | Policy Loss: -3.3984906673431396 | Batches: 125010\nEpisode: 14 | Reward: 450.69108536308374 | Policy Loss: -2.7269558906555176 | Batches: 133344\nEpisode: 15 | Reward: 431.3434379787277 | Policy Loss: -0.2706358730792999 | Batches: 141678\nEpisode: 16 | Reward: 542.9505125009439 | Policy Loss: -0.575009286403656 | Batches: 150012\nEpisode: 17 | Reward: 460.73005509207877 | Policy Loss: 3.6578006744384766 | Batches: 158346\nEpisode: 18 | Reward: 400.0312024065611 | Policy Loss: -12.659549713134766 | Batches: 166680\nEpisode: 19 | Reward: 640.4471802664204 | Policy Loss: -9.147666931152344 | Batches: 175014\nEpisode: 20 | Reward: 397.4319143605095 | Policy Loss: -12.949408531188965 | Batches: 183348\nEpisode: 21 | Reward: 434.07840764614764 | Policy Loss: -18.177019119262695 | Batches: 191682\nEpisode: 22 | Reward: 467.5768550714033 | Policy Loss: 1.988106369972229 | Batches: 200016\nEpisode: 23 | Reward: 1044.8500823400527 | Policy Loss: -17.812744140625 | Batches: 208350\nEpisode: 24 | Reward: 1001.1138615524285 | Policy Loss: -7.568854808807373 | Batches: 216684\nEpisode: 25 | Reward: 2084.3578018166436 | Policy Loss: -21.634923934936523 | Batches: 225018\nEpisode: 26 | Reward: 608.8167621152828 | Policy Loss: -2.300753355026245 | Batches: 233352\nEpisode: 27 | Reward: 787.3442958998464 | Policy Loss: -23.871355056762695 | Batches: 241686\nEpisode: 28 | Reward: 823.3736688192328 | Policy Loss: -5.981899261474609 | Batches: 250020\nEpisode: 29 | Reward: 562.0931173916509 | Policy Loss: -14.894083023071289 | Batches: 258354\nEpisode: 30 | Reward: 730.8450527469511 | Policy Loss: -22.733482360839844 | Batches: 266688\nEpisode: 31 | Reward: 638.9267887342073 | Policy Loss: -32.2351188659668 | Batches: 275022\nEpisode: 32 | Reward: 902.0864191078647 | Policy Loss: -17.632789611816406 | Batches: 283356\nEpisode: 33 | Reward: 823.8791176514036 | Policy Loss: 11.000625610351562 | Batches: 291690\nEpisode: 34 | Reward: 454.7381502377859 | Policy Loss: -28.974525451660156 | Batches: 300024\nEpisode: 35 | Reward: 362.3195219460079 | Policy Loss: -4.258090019226074 | Batches: 308358\nEpisode: 36 | Reward: 794.8086152605848 | Policy Loss: -23.00115394592285 | Batches: 316692\nEpisode: 37 | Reward: 623.5728070520568 | Policy Loss: -16.829208374023438 | Batches: 325026\nEpisode: 38 | Reward: 368.4409692382777 | Policy Loss: -24.4646053314209 | Batches: 333360\nEpisode: 39 | Reward: 694.5039306382278 | Policy Loss: -4.43754768371582 | Batches: 341694\nEpisode: 40 | Reward: 361.23870180834376 | Policy Loss: -20.85945701599121 | Batches: 350028\nEpisode: 41 | Reward: 336.9107380844553 | Policy Loss: -31.121231079101562 | Batches: 358362\nEpisode: 42 | Reward: 654.6325547721156 | Policy Loss: -31.005449295043945 | Batches: 366696\nEpisode: 43 | Reward: 762.8080261553778 | Policy Loss: -2.6483521461486816 | Batches: 375030\nEpisode: 44 | Reward: 331.4413416885746 | Policy Loss: -9.54499340057373 | Batches: 383364\nEpisode: 45 | Reward: 652.0940624021226 | Policy Loss: -26.990509033203125 | Batches: 391698\nEpisode: 46 | Reward: 727.8474399006768 | Policy Loss: -23.644933700561523 | Batches: 400032\nEpisode: 47 | Reward: 657.2939764708692 | Policy Loss: -15.001547813415527 | Batches: 408366\nEpisode: 48 | Reward: 816.2304459304775 | Policy Loss: -24.73237419128418 | Batches: 416700\nEpisode: 49 | Reward: 881.8240062435427 | Policy Loss: 0.8830498456954956 | Batches: 425034\nEpisode: 50 | Reward: 664.0193884437065 | Policy Loss: -16.269350051879883 | Batches: 433368\nEpisode: 51 | Reward: 1874.629980532356 | Policy Loss: -12.907146453857422 | Batches: 441702\nEpisode: 52 | Reward: 1191.7939583611053 | Policy Loss: -9.533597946166992 | Batches: 450036\nEpisode: 53 | Reward: 666.3556063927584 | Policy Loss: -4.834253311157227 | Batches: 458370\nEpisode: 54 | Reward: 1207.3571652324783 | Policy Loss: -15.625155448913574 | Batches: 466704\nEpisode: 55 | Reward: 650.6767104396911 | Policy Loss: -10.100665092468262 | Batches: 475038\nEpisode: 56 | Reward: 719.73618456774 | Policy Loss: -9.07698917388916 | Batches: 483372\nEpisode: 57 | Reward: 2243.2817360834433 | Policy Loss: -30.283130645751953 | Batches: 491706\nEpisode: 58 | Reward: 1139.2123737133675 | Policy Loss: -15.32771110534668 | Batches: 500040\nEpisode: 59 | Reward: 948.9501880473659 | Policy Loss: -20.365894317626953 | Batches: 508374\nEpisode: 60 | Reward: 667.229515739825 | Policy Loss: 1.306848168373108 | Batches: 516708\nEpisode: 61 | Reward: 861.301344428584 | Policy Loss: -4.8520636558532715 | Batches: 525042\nEpisode: 62 | Reward: 768.0910253446973 | Policy Loss: -7.459441661834717 | Batches: 533376\nEpisode: 63 | Reward: 768.6529618274578 | Policy Loss: -26.039976119995117 | Batches: 541710\nEpisode: 64 | Reward: 1052.7084795715937 | Policy Loss: -14.719693183898926 | Batches: 550044\nEpisode: 65 | Reward: 658.0083165494482 | Policy Loss: -13.88230037689209 | Batches: 558378\nEpisode: 66 | Reward: 656.9930602172838 | Policy Loss: -38.46137237548828 | Batches: 566712\nEpisode: 67 | Reward: 722.9525225834149 | Policy Loss: -16.027719497680664 | Batches: 575046\nEpisode: 68 | Reward: 663.3958646177808 | Policy Loss: -21.27385902404785 | Batches: 583380\nEpisode: 69 | Reward: 814.3610137853677 | Policy Loss: 72.45771026611328 | Batches: 591714\nEpisode: 70 | Reward: 653.6456457727916 | Policy Loss: -15.746179580688477 | Batches: 600048\nEpisode: 71 | Reward: 649.7680877013312 | Policy Loss: -29.901498794555664 | Batches: 608382\nEpisode: 72 | Reward: 643.5270712019557 | Policy Loss: -7.15938138961792 | Batches: 616716\nEpisode: 73 | Reward: 2180.8501862686226 | Policy Loss: -22.426937103271484 | Batches: 625050\nEpisode: 74 | Reward: 660.6571577605082 | Policy Loss: -24.82028579711914 | Batches: 633384\nEpisode: 75 | Reward: 658.2219275243231 | Policy Loss: -0.43903759121894836 | Batches: 641718\nEpisode: 76 | Reward: 662.4948369342811 | Policy Loss: -20.911882400512695 | Batches: 650052\nEpisode: 77 | Reward: 669.8686774835904 | Policy Loss: -20.962282180786133 | Batches: 658386\nEpisode: 78 | Reward: 791.7358588104471 | Policy Loss: -27.267589569091797 | Batches: 666720\nEpisode: 79 | Reward: 655.3227144941999 | Policy Loss: -15.653200149536133 | Batches: 675054\nEpisode: 80 | Reward: 648.0498350377435 | Policy Loss: -12.130426406860352 | Batches: 683388\nEpisode: 81 | Reward: 655.5127675413974 | Policy Loss: -9.747262001037598 | Batches: 691722\nEpisode: 82 | Reward: 654.2853278374001 | Policy Loss: -20.29908561706543 | Batches: 700056\nEpisode: 83 | Reward: 1264.1638013379309 | Policy Loss: -20.384992599487305 | Batches: 708390\nEpisode: 84 | Reward: 616.7058958558507 | Policy Loss: -22.370891571044922 | Batches: 716724\nEpisode: 85 | Reward: 646.6180084149753 | Policy Loss: -24.883453369140625 | Batches: 725058\nEpisode: 86 | Reward: 655.7461458403134 | Policy Loss: -14.11231803894043 | Batches: 733392\nEpisode: 87 | Reward: 945.5015621346054 | Policy Loss: -22.299673080444336 | Batches: 741726\nEpisode: 88 | Reward: 2540.0081288939646 | Policy Loss: -25.535198211669922 | Batches: 750060\nEpisode: 89 | Reward: 645.1012012327466 | Policy Loss: -5.672607898712158 | Batches: 758394\nEpisode: 90 | Reward: 662.2255699951214 | Policy Loss: -21.31298065185547 | Batches: 766728\nEpisode: 91 | Reward: 656.8985230203025 | Policy Loss: -18.973064422607422 | Batches: 775062\nEpisode: 92 | Reward: 1455.6564990285367 | Policy Loss: -3.601940631866455 | Batches: 783396\nEpisode: 93 | Reward: 1924.2135769475044 | Policy Loss: -14.779776573181152 | Batches: 791730\nEpisode: 94 | Reward: 1287.6328130715704 | Policy Loss: -25.124216079711914 | Batches: 800064\nEpisode: 95 | Reward: 694.3913126099736 | Policy Loss: -21.209367752075195 | Batches: 808398\nEpisode: 96 | Reward: 614.3796769729848 | Policy Loss: -15.7643404006958 | Batches: 816732\nEpisode: 97 | Reward: 653.7700656107212 | Policy Loss: -17.89053726196289 | Batches: 825066\nEpisode: 98 | Reward: 1275.5790938415407 | Policy Loss: -26.153961181640625 | Batches: 833400\nEpisode: 99 | Reward: 654.3964607218198 | Policy Loss: -0.3306034207344055 | Batches: 841734\nEpisode: 100 | Reward: 663.4548787709739 | Policy Loss: -12.903127670288086 | Batches: 850068\nEpisode: 101 | Reward: 1118.7324298563549 | Policy Loss: -17.81424903869629 | Batches: 858402\nEpisode: 102 | Reward: 605.017783414065 | Policy Loss: -30.78867530822754 | Batches: 866736\nEpisode: 103 | Reward: 909.7928707244213 | Policy Loss: -22.5902156829834 | Batches: 875070\nEpisode: 104 | Reward: 1624.5161634235833 | Policy Loss: -24.134458541870117 | Batches: 883404\nEpisode: 105 | Reward: 558.3965979782586 | Policy Loss: -23.837392807006836 | Batches: 891738\nEpisode: 106 | Reward: 1796.5109601566714 | Policy Loss: -27.435426712036133 | Batches: 900072\nEpisode: 107 | Reward: 740.7142066852768 | Policy Loss: -5.164149284362793 | Batches: 908406\nEpisode: 108 | Reward: 530.8166773454581 | Policy Loss: -34.518211364746094 | Batches: 916740\nEpisode: 109 | Reward: 591.2893293567613 | Policy Loss: -25.779144287109375 | Batches: 925074\nEpisode: 110 | Reward: 826.9752348518307 | Policy Loss: -17.730375289916992 | Batches: 933408\nEpisode: 111 | Reward: 489.27215907742146 | Policy Loss: -22.634782791137695 | Batches: 941742\nEpisode: 112 | Reward: 615.0452932032811 | Policy Loss: -25.951133728027344 | Batches: 950076\nEpisode: 113 | Reward: 1692.3155251390526 | Policy Loss: -27.655004501342773 | Batches: 958410\nEpisode: 114 | Reward: 766.1691052431413 | Policy Loss: -25.801067352294922 | Batches: 966744\nEpisode: 115 | Reward: 477.1457190811267 | Policy Loss: -37.709197998046875 | Batches: 975078\nEpisode: 116 | Reward: 1270.1912371885628 | Policy Loss: -14.175864219665527 | Batches: 983412\nEpisode: 117 | Reward: 828.603360080597 | Policy Loss: -26.267120361328125 | Batches: 991746\nEpisode: 118 | Reward: 439.259193240654 | Policy Loss: -25.002038955688477 | Batches: 1000080\nEpisode: 119 | Reward: 579.1219575817593 | Policy Loss: -20.35959243774414 | Batches: 1008414\nEpisode: 120 | Reward: 600.7870679677148 | Policy Loss: -2.187312602996826 | Batches: 1016748\nEpisode: 121 | Reward: 493.36330443535496 | Policy Loss: -28.80152702331543 | Batches: 1025082\nEpisode: 122 | Reward: 728.8478359018228 | Policy Loss: -41.401092529296875 | Batches: 1033416\nEpisode: 123 | Reward: 1468.5898974869617 | Policy Loss: -24.153337478637695 | Batches: 1041750\nEpisode: 124 | Reward: 426.0039297851581 | Policy Loss: -7.747054100036621 | Batches: 1050084\nEpisode: 125 | Reward: 1921.9561087241486 | Policy Loss: -19.122699737548828 | Batches: 1058418\nEpisode: 126 | Reward: 991.6143421842147 | Policy Loss: -10.81734848022461 | Batches: 1066752\nEpisode: 127 | Reward: 611.6277157914316 | Policy Loss: -20.026395797729492 | Batches: 1075086\nEpisode: 128 | Reward: 314.4588945733702 | Policy Loss: -26.575969696044922 | Batches: 1083420\nEpisode: 129 | Reward: 675.274149716362 | Policy Loss: -35.771705627441406 | Batches: 1091754\nEpisode: 130 | Reward: 311.64584247520565 | Policy Loss: -16.100170135498047 | Batches: 1100088\nEpisode: 131 | Reward: 669.9714746803306 | Policy Loss: -11.801642417907715 | Batches: 1108422\nEpisode: 132 | Reward: 317.46932487586514 | Policy Loss: -38.46589660644531 | Batches: 1116756\nEpisode: 133 | Reward: 642.8227360856391 | Policy Loss: -10.555624008178711 | Batches: 1125090\nEpisode: 134 | Reward: 521.5818932230875 | Policy Loss: -17.33716583251953 | Batches: 1133424\nEpisode: 135 | Reward: 648.4701963472035 | Policy Loss: -15.748994827270508 | Batches: 1141758\nEpisode: 136 | Reward: 406.0475384963694 | Policy Loss: -17.620620727539062 | Batches: 1150092\nEpisode: 137 | Reward: 665.4988170154162 | Policy Loss: -17.59699821472168 | Batches: 1158426\nEpisode: 138 | Reward: 438.02673099480523 | Policy Loss: -11.065742492675781 | Batches: 1166760\nEpisode: 139 | Reward: 561.3708628378541 | Policy Loss: -8.258566856384277 | Batches: 1175094\nEpisode: 140 | Reward: 649.3392740530122 | Policy Loss: -26.59284782409668 | Batches: 1183428\nEpisode: 141 | Reward: 922.5842906621041 | Policy Loss: -3.0751161575317383 | Batches: 1191762\nEpisode: 142 | Reward: 307.2783030759241 | Policy Loss: -12.877828598022461 | Batches: 1200096\nEpisode: 143 | Reward: 307.1936036937842 | Policy Loss: -39.37331008911133 | Batches: 1208430\nEpisode: 144 | Reward: 879.803140434952 | Policy Loss: -10.595709800720215 | Batches: 1216764\nEpisode: 145 | Reward: 962.6574116940734 | Policy Loss: -35.4513053894043 | Batches: 1225098\nEpisode: 146 | Reward: 307.8978403415159 | Policy Loss: -16.540687561035156 | Batches: 1233432\nEpisode: 147 | Reward: 1458.8958402093572 | Policy Loss: 4.2719597816467285 | Batches: 1241766\nEpisode: 148 | Reward: 1780.0482514529072 | Policy Loss: -43.836029052734375 | Batches: 1250100\nEpisode: 149 | Reward: 1957.9072009914028 | Policy Loss: -22.2451171875 | Batches: 1258434\nEpisode: 150 | Reward: 303.41213125829256 | Policy Loss: -21.761110305786133 | Batches: 1266768\nEpisode: 151 | Reward: 305.9742878585521 | Policy Loss: -22.611276626586914 | Batches: 1275102\nEpisode: 152 | Reward: 679.5522054385813 | Policy Loss: -14.758637428283691 | Batches: 1283436\nEpisode: 153 | Reward: 652.9227812010364 | Policy Loss: -16.57782554626465 | Batches: 1291770\nEpisode: 154 | Reward: 501.19604431937415 | Policy Loss: -18.915307998657227 | Batches: 1300104\nEpisode: 155 | Reward: 664.6948324329989 | Policy Loss: -29.545997619628906 | Batches: 1308438\nEpisode: 156 | Reward: 306.74081840977925 | Policy Loss: -25.319494247436523 | Batches: 1316772\nEpisode: 157 | Reward: 529.7270720488827 | Policy Loss: -16.256153106689453 | Batches: 1325106\nEpisode: 158 | Reward: 438.9360045025995 | Policy Loss: -27.530046463012695 | Batches: 1333440\nEpisode: 159 | Reward: 632.4049048444542 | Policy Loss: -14.968634605407715 | Batches: 1341774\nEpisode: 160 | Reward: 452.36523181665734 | Policy Loss: -29.131879806518555 | Batches: 1350108\nEpisode: 161 | Reward: 1311.1794368946628 | Policy Loss: -11.567666053771973 | Batches: 1358442\nEpisode: 162 | Reward: 693.818363507449 | Policy Loss: -17.50423240661621 | Batches: 1366776\nEpisode: 163 | Reward: 301.1345587709523 | Policy Loss: -34.37196350097656 | Batches: 1375110\nEpisode: 164 | Reward: 1041.915048107859 | Policy Loss: -30.88787269592285 | Batches: 1383444\nEpisode: 165 | Reward: 306.76083344989024 | Policy Loss: -29.69403648376465 | Batches: 1391778\nEpisode: 166 | Reward: 884.0915132069324 | Policy Loss: -4.221829414367676 | Batches: 1400112\nEpisode: 167 | Reward: 648.6507262622315 | Policy Loss: -0.6263161301612854 | Batches: 1408446\nEpisode: 168 | Reward: 306.83404872181086 | Policy Loss: -24.22428321838379 | Batches: 1416780\nEpisode: 169 | Reward: 667.2143932457241 | Policy Loss: -20.88295555114746 | Batches: 1425114\nEpisode: 170 | Reward: 1850.2258033492071 | Policy Loss: -29.50050163269043 | Batches: 1433448\nEpisode: 171 | Reward: 662.6821882976601 | Policy Loss: -28.352550506591797 | Batches: 1441782\nEpisode: 172 | Reward: 767.3640010145818 | Policy Loss: -24.128503799438477 | Batches: 1450116\nEpisode: 173 | Reward: 643.3600357209252 | Policy Loss: -20.354333877563477 | Batches: 1458450\nEpisode: 174 | Reward: 576.3626653442454 | Policy Loss: -13.24537181854248 | Batches: 1466784\nEpisode: 175 | Reward: 311.00214936403665 | Policy Loss: -38.15242004394531 | Batches: 1475118\nEpisode: 176 | Reward: 1364.4724775284892 | Policy Loss: -26.042272567749023 | Batches: 1483452\nEpisode: 177 | Reward: 1262.5400905000909 | Policy Loss: -27.1558895111084 | Batches: 1491786\nEpisode: 178 | Reward: 721.5414259560744 | Policy Loss: -29.901336669921875 | Batches: 1500120\nEpisode: 179 | Reward: 317.7465736161031 | Policy Loss: -20.67439079284668 | Batches: 1508454\nEpisode: 180 | Reward: 2417.1009088979326 | Policy Loss: -15.638816833496094 | Batches: 1516788\nEpisode: 181 | Reward: 705.3057575570334 | Policy Loss: -20.057666778564453 | Batches: 1525122\nEpisode: 182 | Reward: 654.0185878791833 | Policy Loss: -4.902451515197754 | Batches: 1533456\nEpisode: 183 | Reward: 655.902269419061 | Policy Loss: -24.491167068481445 | Batches: 1541790\nEpisode: 184 | Reward: 663.2537405380978 | Policy Loss: -13.903068542480469 | Batches: 1550124\nEpisode: 185 | Reward: 943.7151243185941 | Policy Loss: -27.862579345703125 | Batches: 1558458\nEpisode: 186 | Reward: 835.7813938975626 | Policy Loss: -22.576513290405273 | Batches: 1566792\nEpisode: 187 | Reward: 940.2047285205708 | Policy Loss: -9.788774490356445 | Batches: 1575126\nEpisode: 188 | Reward: 1813.2568006815895 | Policy Loss: -15.094491004943848 | Batches: 1583460\nEpisode: 189 | Reward: 633.58788642191 | Policy Loss: -11.820270538330078 | Batches: 1591794\nEpisode: 190 | Reward: 2403.730901055772 | Policy Loss: -18.93852424621582 | Batches: 1600128\nEpisode: 191 | Reward: 1917.2690456440919 | Policy Loss: -19.065214157104492 | Batches: 1608462\nEpisode: 192 | Reward: 1293.144778489745 | Policy Loss: -13.190605163574219 | Batches: 1616796\nEpisode: 193 | Reward: 1026.8139510814417 | Policy Loss: -31.647844314575195 | Batches: 1625130\nEpisode: 194 | Reward: 670.84752601278 | Policy Loss: -16.37404441833496 | Batches: 1633464\nEpisode: 195 | Reward: 664.6398796659709 | Policy Loss: -21.82203483581543 | Batches: 1641798\nEpisode: 196 | Reward: 652.4870957879378 | Policy Loss: -31.232776641845703 | Batches: 1650132\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_1546/917570596.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mdones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mpolicy_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcritic1_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcritic2_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0mbatches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_1546/140017052.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, experiences)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;31m# critic 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic1_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0mcritic1_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_param\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic1_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/wandb/wandb_torch.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(grad)\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_tensor_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_track\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hook_handles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/wandb/wandb_torch.py\u001b[0m in \u001b[0;36m_callback\u001b[0;34m(grad, log_track)\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlog_track_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_track\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_tensor_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_track\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/wandb/wandb_torch.py\u001b[0m in \u001b[0;36mlog_tensor_stats\u001b[0;34m(self, tensor, name)\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtmin\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mtmax\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0mtmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m         \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_bins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m         \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0mbins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_bins\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: max must be larger than min"],"ename":"RuntimeError","evalue":"max must be larger than min","output_type":"error"}]},{"cell_type":"markdown","source":"# **Step 2 :** Install Virtual Display","metadata":{}},{"cell_type":"code","source":"!apt install -y python-opengl ffmpeg > /dev/null 2>&1\n\n# !apt install -y xvfb\n%pip install pyvirtualdisplay","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then import it and define some helper functions:","metadata":{}},{"cell_type":"code","source":"from pyvirtualdisplay import Display\ndisplay = Display(visible=0, size=(400, 300))\ndisplay.start()\n\n\nfrom matplotlib import pyplot as plt, animation\n%matplotlib inline\nfrom IPython import display\n\ndef create_anim(frames, dpi, fps):\n    plt.figure(figsize=(frames[0].shape[1] / dpi, frames[0].shape[0] / dpi), dpi=dpi)\n    patch = plt.imshow(frames[0])\n    def setup():\n        plt.axis('off')\n    def animate(i):\n        patch.set_data(frames[i])\n    anim = animation.FuncAnimation(plt.gcf(), animate, init_func=setup, frames=len(frames), interval=fps)\n    return anim\n\ndef display_anim(frames, dpi=10, fps=100):\n    anim = create_anim(frames, dpi, fps)\n    return anim.to_jshtml()\n\ndef save_anim(frames, filename, dpi=60, fps=50):\n    anim = create_anim(frames, dpi, fps)\n    anim.save(filename)\n\n\nclass trigger:\n    def __init__(self):\n        self._trigger = True\n\n    def __call__(self, e):\n        return self._trigger\n\n    def set(self, t):\n        self._trigger = t","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Display Episode Frames In the notebook","metadata":{}},{"cell_type":"markdown","source":"Finally render an episode and display it in the notebook :","metadata":{}},{"cell_type":"code","source":"frames = []\n#env = gym.make(\"Walker2d-v2\")\nenv = gym.make('hopper-expert-v2') \nobs = env.reset()\ndone = False\nepisode_reward = 0\nwhile not done:\n    frames.append(env.render(mode='rgb_array'))\n    obs, rew,done,info = env.step(agent.get_action(obs, eval=True))\n    episode_reward += rew\nenv.close()\n\nprint(\"Episode Reward: \" + str(episode_reward))\n\ndisplay.HTML(display_anim(frames))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}