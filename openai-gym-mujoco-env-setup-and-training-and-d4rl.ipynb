{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30152,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n\nI needed to setup OpenAI Gym Mujuco Envroinments for my personal project, but unfortunately I did not find any solution that works properly, or some of the solutions that used to work are no longer working.\nSo by searching in the internet, github and exploring previous netbooks, I was able to find a way to implement it in Kegel.\n\nIn this notebook, we first setup Mujuco physics engine then we go for setting up a virtual display to display the environments episodes in the notebook and finally we apply reinforcement learning using OpenAI stable baselines v2 to train our agent ","metadata":{}},{"cell_type":"markdown","source":"![OpenAI Mujuco Environments](https://i.imgur.com/FM4B4Gq.png)","metadata":{}},{"cell_type":"markdown","source":"# **Please Do Upvote :D**","metadata":{}},{"cell_type":"markdown","source":"# **Step 1 :** Install Mujuco Physics engine on the kernel","metadata":{}},{"cell_type":"code","source":"import os\nif not os.path.exists('.mujoco_setup_complete'):\n  # Get the prereqs\n  !apt-get -qq update\n  !apt-get -qq install -y libosmesa6-dev libgl1-mesa-glx libglfw3 libgl1-mesa-dev libglew-dev patchelf\n  # Get Mujoco\n  !mkdir ~/.mujoco\n  !wget -q https://mujoco.org/download/mujoco210-linux-x86_64.tar.gz -O mujoco.tar.gz\n  !tar -zxf mujoco.tar.gz -C \"$HOME/.mujoco\"\n  !rm mujoco.tar.gz\n  # Add it to the actively loaded path and the bashrc path (these only do so much)\n  !echo 'export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$HOME/.mujoco/mujoco210/bin' >> ~/.bashrc \n  !echo 'export LD_PRELOAD=$LD_PRELOAD:/usr/lib/x86_64-linux-gnu/libGLEW.so' >> ~/.bashrc \n  # THE ANNOYING ONE, FORCE IT INTO LDCONFIG SO WE ACTUALLY GET ACCESS TO IT THIS SESSION\n  !echo \"/root/.mujoco/mujoco210/bin\" > /etc/ld.so.conf.d/mujoco_ld_lib_path.conf\n  !ldconfig\n  # Install Mujoco-py\n  !pip3 install -U 'mujoco-py<2.2,>=2.1'\n  # run once\n  !touch .mujoco_setup_complete\n\ntry:\n  if _mujoco_run_once:\n    pass\nexcept NameError:\n  _mujoco_run_once = False\nif not _mujoco_run_once:\n  # Add it to the actively loaded path and the bashrc path (these only do so much)\n  try:\n    os.environ['LD_LIBRARY_PATH']=os.environ['LD_LIBRARY_PATH'] + ':/root/.mujoco/mujoco210/bin'\n  except KeyError:\n    os.environ['LD_LIBRARY_PATH']='/root/.mujoco/mujoco210/bin'\n  try:\n    os.environ['LD_PRELOAD']=os.environ['LD_PRELOAD'] + ':/usr/lib/x86_64-linux-gnu/libGLEW.so'\n  except KeyError:\n    os.environ['LD_PRELOAD']='/usr/lib/x86_64-linux-gnu/libGLEW.so'\n  # presetup so we don't see output on first env initialization\n  import mujoco_py\n  _mujoco_run_once = True\n#source of this code block : https://gist.github.com/BuildingAtom/3119ac9c595324c8001a7454f23bf8c8","metadata":{"execution":{"iopub.status.busy":"2024-08-14T12:38:06.188967Z","iopub.execute_input":"2024-08-14T12:38:06.189203Z","iopub.status.idle":"2024-08-14T12:40:14.643730Z","shell.execute_reply.started":"2024-08-14T12:38:06.189133Z","shell.execute_reply":"2024-08-14T12:40:14.642944Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"W: An error occurred during the signature verification. The repository is not updated and the previous index files will be used. GPG error: http://packages.cloud.google.com/apt gcsfuse-focal InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY C0BA5CE6DC6315A3\nW: An error occurred during the signature verification. The repository is not updated and the previous index files will be used. GPG error: https://packages.cloud.google.com/apt cloud-sdk InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY C0BA5CE6DC6315A3\nW: An error occurred during the signature verification. The repository is not updated and the previous index files will be used. GPG error: https://packages.cloud.google.com/apt google-fast-socket InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY C0BA5CE6DC6315A3\nW: GPG error: https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY A4B469963BF863CC\nE: The repository 'https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease' is no longer signed.\nExtracting templates from packages: 100%\n(Reading database ... 102169 files and directories currently installed.)\nPreparing to unpack .../00-libx11-6_2%3a1.6.9-2ubuntu1.6_amd64.deb ...\nUnpacking libx11-6:amd64 (2:1.6.9-2ubuntu1.6) over (2:1.6.9-2ubuntu1.2) ...\nPreparing to unpack .../01-libgl1-mesa-dri_21.2.6-0ubuntu0.1~20.04.2_amd64.deb ...\nUnpacking libgl1-mesa-dri:amd64 (21.2.6-0ubuntu0.1~20.04.2) over (21.0.3-0ubuntu0.3~20.04.4) ...\nPreparing to unpack .../02-libglx-mesa0_21.2.6-0ubuntu0.1~20.04.2_amd64.deb ...\nUnpacking libglx-mesa0:amd64 (21.2.6-0ubuntu0.1~20.04.2) over (21.0.3-0ubuntu0.3~20.04.4) ...\nPreparing to unpack .../03-libglapi-mesa_21.2.6-0ubuntu0.1~20.04.2_amd64.deb ...\nUnpacking libglapi-mesa:amd64 (21.2.6-0ubuntu0.1~20.04.2) over (21.0.3-0ubuntu0.3~20.04.4) ...\nPreparing to unpack .../04-libgl1_1.3.2-1~ubuntu0.20.04.2_amd64.deb ...\nUnpacking libgl1:amd64 (1.3.2-1~ubuntu0.20.04.2) over (1.3.2-1~ubuntu0.20.04.1) ...\nPreparing to unpack .../05-libglx0_1.3.2-1~ubuntu0.20.04.2_amd64.deb ...\nUnpacking libglx0:amd64 (1.3.2-1~ubuntu0.20.04.2) over (1.3.2-1~ubuntu0.20.04.1) ...\nPreparing to unpack .../06-libglvnd0_1.3.2-1~ubuntu0.20.04.2_amd64.deb ...\nUnpacking libglvnd0:amd64 (1.3.2-1~ubuntu0.20.04.2) over (1.3.2-1~ubuntu0.20.04.1) ...\nSelecting previously unselected package libwayland-server0:amd64.\nPreparing to unpack .../07-libwayland-server0_1.18.0-1ubuntu0.1_amd64.deb ...\nUnpacking libwayland-server0:amd64 (1.18.0-1ubuntu0.1) ...\nSelecting previously unselected package libgbm1:amd64.\nPreparing to unpack .../08-libgbm1_21.2.6-0ubuntu0.1~20.04.2_amd64.deb ...\nUnpacking libgbm1:amd64 (21.2.6-0ubuntu0.1~20.04.2) ...\nSelecting previously unselected package libegl-mesa0:amd64.\nPreparing to unpack .../09-libegl-mesa0_21.2.6-0ubuntu0.1~20.04.2_amd64.deb ...\nUnpacking libegl-mesa0:amd64 (21.2.6-0ubuntu0.1~20.04.2) ...\nSelecting previously unselected package libegl1:amd64.\nPreparing to unpack .../10-libegl1_1.3.2-1~ubuntu0.20.04.2_amd64.deb ...\nUnpacking libegl1:amd64 (1.3.2-1~ubuntu0.20.04.2) ...\nSelecting previously unselected package xorg-sgml-doctools.\nPreparing to unpack .../11-xorg-sgml-doctools_1%3a1.11-1_all.deb ...\nUnpacking xorg-sgml-doctools (1:1.11-1) ...\nSelecting previously unselected package x11proto-dev.\nPreparing to unpack .../12-x11proto-dev_2019.2-1ubuntu1_all.deb ...\nUnpacking x11proto-dev (2019.2-1ubuntu1) ...\nSelecting previously unselected package x11proto-core-dev.\nPreparing to unpack .../13-x11proto-core-dev_2019.2-1ubuntu1_all.deb ...\nUnpacking x11proto-core-dev (2019.2-1ubuntu1) ...\nSelecting previously unselected package libxau-dev:amd64.\nPreparing to unpack .../14-libxau-dev_1%3a1.0.9-0ubuntu1_amd64.deb ...\nUnpacking libxau-dev:amd64 (1:1.0.9-0ubuntu1) ...\nSelecting previously unselected package libxdmcp-dev:amd64.\nPreparing to unpack .../15-libxdmcp-dev_1%3a1.1.3-0ubuntu1_amd64.deb ...\nUnpacking libxdmcp-dev:amd64 (1:1.1.3-0ubuntu1) ...\nSelecting previously unselected package xtrans-dev.\nPreparing to unpack .../16-xtrans-dev_1.4.0-1_all.deb ...\nUnpacking xtrans-dev (1.4.0-1) ...\nSelecting previously unselected package libpthread-stubs0-dev:amd64.\nPreparing to unpack .../17-libpthread-stubs0-dev_0.4-1_amd64.deb ...\nUnpacking libpthread-stubs0-dev:amd64 (0.4-1) ...\nSelecting previously unselected package libxcb1-dev:amd64.\nPreparing to unpack .../18-libxcb1-dev_1.14-2_amd64.deb ...\nUnpacking libxcb1-dev:amd64 (1.14-2) ...\nSelecting previously unselected package libx11-dev:amd64.\nPreparing to unpack .../19-libx11-dev_2%3a1.6.9-2ubuntu1.6_amd64.deb ...\nUnpacking libx11-dev:amd64 (2:1.6.9-2ubuntu1.6) ...\nSelecting previously unselected package libglx-dev:amd64.\nPreparing to unpack .../20-libglx-dev_1.3.2-1~ubuntu0.20.04.2_amd64.deb ...\nUnpacking libglx-dev:amd64 (1.3.2-1~ubuntu0.20.04.2) ...\nSelecting previously unselected package libgl-dev:amd64.\nPreparing to unpack .../21-libgl-dev_1.3.2-1~ubuntu0.20.04.2_amd64.deb ...\nUnpacking libgl-dev:amd64 (1.3.2-1~ubuntu0.20.04.2) ...\nSelecting previously unselected package libegl-dev:amd64.\nPreparing to unpack .../22-libegl-dev_1.3.2-1~ubuntu0.20.04.2_amd64.deb ...\nUnpacking libegl-dev:amd64 (1.3.2-1~ubuntu0.20.04.2) ...\nPreparing to unpack .../23-libgl1-mesa-glx_21.2.6-0ubuntu0.1~20.04.2_amd64.deb ...\nUnpacking libgl1-mesa-glx:amd64 (21.2.6-0ubuntu0.1~20.04.2) over (21.0.3-0ubuntu0.3~20.04.4) ...\nSelecting previously unselected package libgles1:amd64.\nPreparing to unpack .../24-libgles1_1.3.2-1~ubuntu0.20.04.2_amd64.deb ...\nUnpacking libgles1:amd64 (1.3.2-1~ubuntu0.20.04.2) ...\nSelecting previously unselected package libgles2:amd64.\nPreparing to unpack .../25-libgles2_1.3.2-1~ubuntu0.20.04.2_amd64.deb ...\nUnpacking libgles2:amd64 (1.3.2-1~ubuntu0.20.04.2) ...\nSelecting previously unselected package libgles-dev:amd64.\nPreparing to unpack .../26-libgles-dev_1.3.2-1~ubuntu0.20.04.2_amd64.deb ...\nUnpacking libgles-dev:amd64 (1.3.2-1~ubuntu0.20.04.2) ...\nSelecting previously unselected package libopengl0:amd64.\nPreparing to unpack .../27-libopengl0_1.3.2-1~ubuntu0.20.04.2_amd64.deb ...\nUnpacking libopengl0:amd64 (1.3.2-1~ubuntu0.20.04.2) ...\nSelecting previously unselected package libopengl-dev:amd64.\nPreparing to unpack .../28-libopengl-dev_1.3.2-1~ubuntu0.20.04.2_amd64.deb ...\nUnpacking libopengl-dev:amd64 (1.3.2-1~ubuntu0.20.04.2) ...\nSelecting previously unselected package libglvnd-dev:amd64.\nPreparing to unpack .../29-libglvnd-dev_1.3.2-1~ubuntu0.20.04.2_amd64.deb ...\nUnpacking libglvnd-dev:amd64 (1.3.2-1~ubuntu0.20.04.2) ...\nSelecting previously unselected package libgl1-mesa-dev:amd64.\nPreparing to unpack .../30-libgl1-mesa-dev_21.2.6-0ubuntu0.1~20.04.2_amd64.deb ...\nUnpacking libgl1-mesa-dev:amd64 (21.2.6-0ubuntu0.1~20.04.2) ...\nSelecting previously unselected package libglew2.1:amd64.\nPreparing to unpack .../31-libglew2.1_2.1.0-4_amd64.deb ...\nUnpacking libglew2.1:amd64 (2.1.0-4) ...\nSelecting previously unselected package libglu1-mesa:amd64.\nPreparing to unpack .../32-libglu1-mesa_9.0.1-1build1_amd64.deb ...\nUnpacking libglu1-mesa:amd64 (9.0.1-1build1) ...\nSelecting previously unselected package libglu1-mesa-dev:amd64.\nPreparing to unpack .../33-libglu1-mesa-dev_9.0.1-1build1_amd64.deb ...\nUnpacking libglu1-mesa-dev:amd64 (9.0.1-1build1) ...\nSelecting previously unselected package libglew-dev:amd64.\nPreparing to unpack .../34-libglew-dev_2.1.0-4_amd64.deb ...\nUnpacking libglew-dev:amd64 (2.1.0-4) ...\nSelecting previously unselected package libglfw3:amd64.\nPreparing to unpack .../35-libglfw3_3.3.2-1_amd64.deb ...\nUnpacking libglfw3:amd64 (3.3.2-1) ...\nSelecting previously unselected package patchelf.\nPreparing to unpack .../36-patchelf_0.10-2build1_amd64.deb ...\nUnpacking patchelf (0.10-2build1) ...\nSelecting previously unselected package libosmesa6:amd64.\nPreparing to unpack .../37-libosmesa6_21.2.6-0ubuntu0.1~20.04.2_amd64.deb ...\nUnpacking libosmesa6:amd64 (21.2.6-0ubuntu0.1~20.04.2) ...\nSelecting previously unselected package libosmesa6-dev:amd64.\nPreparing to unpack .../38-libosmesa6-dev_21.2.6-0ubuntu0.1~20.04.2_amd64.deb ...\nUnpacking libosmesa6-dev:amd64 (21.2.6-0ubuntu0.1~20.04.2) ...\nSetting up libwayland-server0:amd64 (1.18.0-1ubuntu0.1) ...\nSetting up libgbm1:amd64 (21.2.6-0ubuntu0.1~20.04.2) ...\nSetting up libglvnd0:amd64 (1.3.2-1~ubuntu0.20.04.2) ...\nSetting up libpthread-stubs0-dev:amd64 (0.4-1) ...\nSetting up libopengl0:amd64 (1.3.2-1~ubuntu0.20.04.2) ...\nSetting up xtrans-dev (1.4.0-1) ...\nSetting up libgles2:amd64 (1.3.2-1~ubuntu0.20.04.2) ...\nSetting up libgles1:amd64 (1.3.2-1~ubuntu0.20.04.2) ...\nSetting up libglapi-mesa:amd64 (21.2.6-0ubuntu0.1~20.04.2) ...\nSetting up libx11-6:amd64 (2:1.6.9-2ubuntu1.6) ...\nSetting up xorg-sgml-doctools (1:1.11-1) ...\nSetting up libopengl-dev:amd64 (1.3.2-1~ubuntu0.20.04.2) ...\nSetting up patchelf (0.10-2build1) ...\nSetting up libosmesa6:amd64 (21.2.6-0ubuntu0.1~20.04.2) ...\nSetting up x11proto-dev (2019.2-1ubuntu1) ...\nSetting up libxau-dev:amd64 (1:1.0.9-0ubuntu1) ...\nSetting up libgl1-mesa-dri:amd64 (21.2.6-0ubuntu0.1~20.04.2) ...\nSetting up libxdmcp-dev:amd64 (1:1.1.3-0ubuntu1) ...\nSetting up libegl-mesa0:amd64 (21.2.6-0ubuntu0.1~20.04.2) ...\nSetting up x11proto-core-dev (2019.2-1ubuntu1) ...\nSetting up libegl1:amd64 (1.3.2-1~ubuntu0.20.04.2) ...\nSetting up libglx-mesa0:amd64 (21.2.6-0ubuntu0.1~20.04.2) ...\nSetting up libxcb1-dev:amd64 (1.14-2) ...\nSetting up libglx0:amd64 (1.3.2-1~ubuntu0.20.04.2) ...\nSetting up libx11-dev:amd64 (2:1.6.9-2ubuntu1.6) ...\nSetting up libgl1:amd64 (1.3.2-1~ubuntu0.20.04.2) ...\nSetting up libgl1-mesa-glx:amd64 (21.2.6-0ubuntu0.1~20.04.2) ...\nSetting up libglew2.1:amd64 (2.1.0-4) ...\nSetting up libglx-dev:amd64 (1.3.2-1~ubuntu0.20.04.2) ...\nSetting up libglu1-mesa:amd64 (9.0.1-1build1) ...\nSetting up libgl-dev:amd64 (1.3.2-1~ubuntu0.20.04.2) ...\nSetting up libglfw3:amd64 (3.3.2-1) ...\nSetting up libegl-dev:amd64 (1.3.2-1~ubuntu0.20.04.2) ...\nSetting up libglu1-mesa-dev:amd64 (9.0.1-1build1) ...\nSetting up libosmesa6-dev:amd64 (21.2.6-0ubuntu0.1~20.04.2) ...\nSetting up libgles-dev:amd64 (1.3.2-1~ubuntu0.20.04.2) ...\nSetting up libglvnd-dev:amd64 (1.3.2-1~ubuntu0.20.04.2) ...\nSetting up libglew-dev:amd64 (2.1.0-4) ...\nSetting up libgl1-mesa-dev:amd64 (21.2.6-0ubuntu0.1~20.04.2) ...\nProcessing triggers for libc-bin (2.31-0ubuntu9.2) ...\nCollecting mujoco-py<2.2,>=2.1\n  Downloading mujoco_py-2.1.2.14-py3-none-any.whl (2.4 MB)\n     |████████████████████████████████| 2.4 MB 1.2 MB/s            \n\u001b[?25hRequirement already satisfied: cffi>=1.10 in /opt/conda/lib/python3.7/site-packages (from mujoco-py<2.2,>=2.1) (1.15.0)\nCollecting glfw>=1.4.0\n  Downloading glfw-2.7.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl (211 kB)\n     |████████████████████████████████| 211 kB 64.8 MB/s            \n\u001b[?25hRequirement already satisfied: fasteners~=0.15 in /opt/conda/lib/python3.7/site-packages (from mujoco-py<2.2,>=2.1) (0.16.3)\nRequirement already satisfied: Cython>=0.27.2 in /opt/conda/lib/python3.7/site-packages (from mujoco-py<2.2,>=2.1) (0.29.24)\nRequirement already satisfied: numpy>=1.11 in /opt/conda/lib/python3.7/site-packages (from mujoco-py<2.2,>=2.1) (1.19.5)\nRequirement already satisfied: imageio>=2.1.2 in /opt/conda/lib/python3.7/site-packages (from mujoco-py<2.2,>=2.1) (2.9.0)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.10->mujoco-py<2.2,>=2.1) (2.21)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from fasteners~=0.15->mujoco-py<2.2,>=2.1) (1.16.0)\nRequirement already satisfied: pillow in /opt/conda/lib/python3.7/site-packages (from imageio>=2.1.2->mujoco-py<2.2,>=2.1) (8.2.0)\nInstalling collected packages: glfw, mujoco-py\nSuccessfully installed glfw-2.7.0 mujoco-py-2.1.2.14\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\nCompiling /opt/conda/lib/python3.7/site-packages/mujoco_py/cymj.pyx because it depends on /opt/conda/lib/python3.7/site-packages/mujoco_py/pxd/mujoco.pxd.\n[1/1] Cythonizing /opt/conda/lib/python3.7/site-packages/mujoco_py/cymj.pyx\nrunning build_ext\nbuilding 'mujoco_py.cymj' extension\ncreating /opt/conda/lib/python3.7/site-packages/mujoco_py/generated/_pyxbld_2.1.2.14_37_linuxgpuextensionbuilder\ncreating /opt/conda/lib/python3.7/site-packages/mujoco_py/generated/_pyxbld_2.1.2.14_37_linuxgpuextensionbuilder/temp.linux-x86_64-3.7\ncreating /opt/conda/lib/python3.7/site-packages/mujoco_py/generated/_pyxbld_2.1.2.14_37_linuxgpuextensionbuilder/temp.linux-x86_64-3.7/opt\ncreating /opt/conda/lib/python3.7/site-packages/mujoco_py/generated/_pyxbld_2.1.2.14_37_linuxgpuextensionbuilder/temp.linux-x86_64-3.7/opt/conda\ncreating /opt/conda/lib/python3.7/site-packages/mujoco_py/generated/_pyxbld_2.1.2.14_37_linuxgpuextensionbuilder/temp.linux-x86_64-3.7/opt/conda/lib\ncreating /opt/conda/lib/python3.7/site-packages/mujoco_py/generated/_pyxbld_2.1.2.14_37_linuxgpuextensionbuilder/temp.linux-x86_64-3.7/opt/conda/lib/python3.7\ncreating /opt/conda/lib/python3.7/site-packages/mujoco_py/generated/_pyxbld_2.1.2.14_37_linuxgpuextensionbuilder/temp.linux-x86_64-3.7/opt/conda/lib/python3.7/site-packages\ncreating /opt/conda/lib/python3.7/site-packages/mujoco_py/generated/_pyxbld_2.1.2.14_37_linuxgpuextensionbuilder/temp.linux-x86_64-3.7/opt/conda/lib/python3.7/site-packages/mujoco_py\ncreating /opt/conda/lib/python3.7/site-packages/mujoco_py/generated/_pyxbld_2.1.2.14_37_linuxgpuextensionbuilder/temp.linux-x86_64-3.7/opt/conda/lib/python3.7/site-packages/mujoco_py/gl\ngcc -pthread -B /opt/conda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/opt/conda/lib/python3.7/site-packages/mujoco_py -I/root/.mujoco/mujoco210/include -I/opt/conda/lib/python3.7/site-packages/numpy/core/include -I/opt/conda/lib/python3.7/site-packages/mujoco_py/vendor/egl -I/opt/conda/include/python3.7m -c /opt/conda/lib/python3.7/site-packages/mujoco_py/cymj.c -o /opt/conda/lib/python3.7/site-packages/mujoco_py/generated/_pyxbld_2.1.2.14_37_linuxgpuextensionbuilder/temp.linux-x86_64-3.7/opt/conda/lib/python3.7/site-packages/mujoco_py/cymj.o -fopenmp -w\ngcc -pthread -B /opt/conda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/opt/conda/lib/python3.7/site-packages/mujoco_py -I/root/.mujoco/mujoco210/include -I/opt/conda/lib/python3.7/site-packages/numpy/core/include -I/opt/conda/lib/python3.7/site-packages/mujoco_py/vendor/egl -I/opt/conda/include/python3.7m -c /opt/conda/lib/python3.7/site-packages/mujoco_py/gl/eglshim.c -o /opt/conda/lib/python3.7/site-packages/mujoco_py/generated/_pyxbld_2.1.2.14_37_linuxgpuextensionbuilder/temp.linux-x86_64-3.7/opt/conda/lib/python3.7/site-packages/mujoco_py/gl/eglshim.o -fopenmp -w\ncreating /opt/conda/lib/python3.7/site-packages/mujoco_py/generated/_pyxbld_2.1.2.14_37_linuxgpuextensionbuilder/lib.linux-x86_64-3.7\ncreating /opt/conda/lib/python3.7/site-packages/mujoco_py/generated/_pyxbld_2.1.2.14_37_linuxgpuextensionbuilder/lib.linux-x86_64-3.7/mujoco_py\ngcc -pthread -shared -B /opt/conda/compiler_compat -L/opt/conda/lib -Wl,-rpath=/opt/conda/lib -Wl,--no-as-needed -Wl,--sysroot=/ /opt/conda/lib/python3.7/site-packages/mujoco_py/generated/_pyxbld_2.1.2.14_37_linuxgpuextensionbuilder/temp.linux-x86_64-3.7/opt/conda/lib/python3.7/site-packages/mujoco_py/cymj.o /opt/conda/lib/python3.7/site-packages/mujoco_py/generated/_pyxbld_2.1.2.14_37_linuxgpuextensionbuilder/temp.linux-x86_64-3.7/opt/conda/lib/python3.7/site-packages/mujoco_py/gl/eglshim.o -L/root/.mujoco/mujoco210/bin -Wl,-R/root/.mujoco/mujoco210/bin -lmujoco210 -lglewegl -o /opt/conda/lib/python3.7/site-packages/mujoco_py/generated/_pyxbld_2.1.2.14_37_linuxgpuextensionbuilder/lib.linux-x86_64-3.7/mujoco_py/cymj.cpython-37m-x86_64-linux-gnu.so -fopenmp\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Then import it :","metadata":{}},{"cell_type":"code","source":"import mujoco_py","metadata":{"execution":{"iopub.status.busy":"2024-08-14T12:40:14.645494Z","iopub.execute_input":"2024-08-14T12:40:14.645759Z","iopub.status.idle":"2024-08-14T12:40:14.650072Z","shell.execute_reply.started":"2024-08-14T12:40:14.645728Z","shell.execute_reply":"2024-08-14T12:40:14.649317Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## install D4RL","metadata":{}},{"cell_type":"code","source":"# intall d4rl github repo\n!pip install git+https://github.com/Farama-Foundation/d4rl@master#egg=d4rl","metadata":{"execution":{"iopub.status.busy":"2024-08-14T12:40:14.651109Z","iopub.execute_input":"2024-08-14T12:40:14.651305Z","iopub.status.idle":"2024-08-14T12:40:56.414474Z","shell.execute_reply.started":"2024-08-14T12:40:14.651280Z","shell.execute_reply":"2024-08-14T12:40:56.413526Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting d4rl\n  Cloning https://github.com/Farama-Foundation/d4rl (to revision master) to /tmp/pip-install-z_tqgg_j/d4rl_6e3b33408ea94f989feaca850038073a\n  Running command git clone --filter=blob:none -q https://github.com/Farama-Foundation/d4rl /tmp/pip-install-z_tqgg_j/d4rl_6e3b33408ea94f989feaca850038073a\n  Resolved https://github.com/Farama-Foundation/d4rl to commit 71a9549f2091accff93eeff68f1f3ab2c0e0a288\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting mjrl@ git+https://github.com/aravindr93/mjrl@master#egg=mjrl\n  Cloning https://github.com/aravindr93/mjrl (to revision master) to /tmp/pip-install-z_tqgg_j/mjrl_9bc225f986ac4077b07bb9c1450c7fac\n  Running command git clone --filter=blob:none -q https://github.com/aravindr93/mjrl /tmp/pip-install-z_tqgg_j/mjrl_9bc225f986ac4077b07bb9c1450c7fac\n  Resolved https://github.com/aravindr93/mjrl to commit 3871d93763d3b49c4741e6daeaebbc605fe140dc\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: gym<0.24.0 in /opt/conda/lib/python3.7/site-packages (from d4rl) (0.21.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from d4rl) (1.19.5)\nRequirement already satisfied: mujoco_py in /opt/conda/lib/python3.7/site-packages (from d4rl) (2.1.2.14)\nCollecting pybullet\n  Downloading pybullet-3.2.6-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (103.2 MB)\n     |████████████████████████████████| 103.2 MB 14 kB/s              \n\u001b[?25hRequirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (from d4rl) (3.1.0)\nRequirement already satisfied: termcolor in /opt/conda/lib/python3.7/site-packages (from d4rl) (1.1.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from d4rl) (8.0.3)\nCollecting dm_control>=1.0.3\n  Downloading dm_control-1.0.13-py3-none-any.whl (39.3 MB)\n     |████████████████████████████████| 39.3 MB 53.6 MB/s            \n\u001b[?25hCollecting mujoco>=2.3.6\n  Downloading mujoco-2.3.6-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n     |████████████████████████████████| 4.5 MB 56.5 MB/s            \n\u001b[?25hRequirement already satisfied: pyparsing>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from dm_control>=1.0.3->d4rl) (3.0.6)\nCollecting labmaze\n  Downloading labmaze-1.0.6-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n     |████████████████████████████████| 4.9 MB 56.4 MB/s            \n\u001b[?25hRequirement already satisfied: glfw in /opt/conda/lib/python3.7/site-packages (from dm_control>=1.0.3->d4rl) (2.7.0)\nRequirement already satisfied: absl-py>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from dm_control>=1.0.3->d4rl) (0.15.0)\nCollecting protobuf>=3.19.4\n  Downloading protobuf-4.24.4-cp37-abi3-manylinux2014_x86_64.whl (311 kB)\n     |████████████████████████████████| 311 kB 55.4 MB/s            \n\u001b[?25hRequirement already satisfied: setuptools!=50.0.0 in /opt/conda/lib/python3.7/site-packages (from dm_control>=1.0.3->d4rl) (59.1.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from dm_control>=1.0.3->d4rl) (4.62.3)\nCollecting pyopengl>=3.1.4\n  Downloading PyOpenGL-3.1.7-py3-none-any.whl (2.4 MB)\n     |████████████████████████████████| 2.4 MB 36.6 MB/s            \n\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from dm_control>=1.0.3->d4rl) (2.25.1)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.7/site-packages (from dm_control>=1.0.3->d4rl) (4.6.4)\nCollecting dm-env\n  Downloading dm_env-1.6-py3-none-any.whl (26 kB)\nRequirement already satisfied: dm-tree!=0.1.2 in /opt/conda/lib/python3.7/site-packages (from dm_control>=1.0.3->d4rl) (0.1.6)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from dm_control>=1.0.3->d4rl) (1.7.2)\nRequirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from gym<0.24.0->d4rl) (2.0.0)\nRequirement already satisfied: importlib-metadata>=4.8.1 in /opt/conda/lib/python3.7/site-packages (from gym<0.24.0->d4rl) (4.8.2)\nRequirement already satisfied: cached-property in /opt/conda/lib/python3.7/site-packages (from h5py->d4rl) (1.5.2)\nRequirement already satisfied: fasteners~=0.15 in /opt/conda/lib/python3.7/site-packages (from mujoco_py->d4rl) (0.16.3)\nRequirement already satisfied: cffi>=1.10 in /opt/conda/lib/python3.7/site-packages (from mujoco_py->d4rl) (1.15.0)\nRequirement already satisfied: Cython>=0.27.2 in /opt/conda/lib/python3.7/site-packages (from mujoco_py->d4rl) (0.29.24)\nRequirement already satisfied: imageio>=2.1.2 in /opt/conda/lib/python3.7/site-packages (from mujoco_py->d4rl) (2.9.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from absl-py>=0.7.0->dm_control>=1.0.3->d4rl) (1.16.0)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.10->mujoco_py->d4rl) (2.21)\nRequirement already satisfied: pillow in /opt/conda/lib/python3.7/site-packages (from imageio>=2.1.2->mujoco_py->d4rl) (8.2.0)\nRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.8.1->gym<0.24.0->d4rl) (3.10.0.2)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.8.1->gym<0.24.0->d4rl) (3.6.0)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->dm_control>=1.0.3->d4rl) (2.10)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->dm_control>=1.0.3->d4rl) (2021.10.8)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->dm_control>=1.0.3->d4rl) (1.26.7)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->dm_control>=1.0.3->d4rl) (4.0.0)\nBuilding wheels for collected packages: d4rl, mjrl\n  Building wheel for d4rl (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for d4rl: filename=D4RL-1.1-py3-none-any.whl size=26432223 sha256=70482a8272136e8177d520f8ab0f92d581334df08a041ba097a9be9954ee77d6\n  Stored in directory: /tmp/pip-ephem-wheel-cache-5kor4s8f/wheels/e3/6a/07/5d467cea78b4ba39297b0c15128d19e98a88f5bfa00c7d48f8\n  Building wheel for mjrl (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for mjrl: filename=mjrl-1.0.0-py3-none-any.whl size=61953 sha256=bd217d8c5da1f9ed76c0b946aec1534405a8bda4fffc9d96650086211920897e\n  Stored in directory: /tmp/pip-ephem-wheel-cache-5kor4s8f/wheels/93/b7/9c/081e42bc27b633708ab9f5b757ffb500b5c0cd2ca52b75b9b0\nSuccessfully built d4rl mjrl\nInstalling collected packages: pyopengl, protobuf, mujoco, labmaze, dm-env, pybullet, mjrl, dm-control, d4rl\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 3.19.1\n    Uninstalling protobuf-3.19.1:\n      Successfully uninstalled protobuf-3.19.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-io 0.21.0 requires tensorflow-io-gcs-filesystem==0.21.0, which is not installed.\nexplainable-ai-sdk 1.3.2 requires xai-image-widget, which is not installed.\ncudf 21.10.1 requires cupy-cuda110, which is not installed.\nbeatrix-jupyterlab 3.1.4 requires google-cloud-bigquery-storage, which is not installed.\ntfx-bsl 1.4.0 requires absl-py<0.13,>=0.9, but you have absl-py 0.15.0 which is incompatible.\ntfx-bsl 1.4.0 requires protobuf<4,>=3.13, but you have protobuf 4.24.4 which is incompatible.\ntfx-bsl 1.4.0 requires pyarrow<6,>=1, but you have pyarrow 6.0.0 which is incompatible.\ntensorflow 2.6.2 requires six~=1.15.0, but you have six 1.16.0 which is incompatible.\ntensorflow 2.6.2 requires typing-extensions~=3.7.4, but you have typing-extensions 3.10.0.2 which is incompatible.\ntensorflow 2.6.2 requires wrapt~=1.12.1, but you have wrapt 1.13.3 which is incompatible.\ntensorflow-transform 1.4.0 requires absl-py<0.13,>=0.9, but you have absl-py 0.15.0 which is incompatible.\ntensorflow-transform 1.4.0 requires protobuf<4,>=3.13, but you have protobuf 4.24.4 which is incompatible.\ntensorflow-transform 1.4.0 requires pyarrow<6,>=1, but you have pyarrow 6.0.0 which is incompatible.\ntensorflow-metadata 1.4.0 requires absl-py<0.13,>=0.9, but you have absl-py 0.15.0 which is incompatible.\ntensorflow-metadata 1.4.0 requires protobuf<4,>=3.13, but you have protobuf 4.24.4 which is incompatible.\nmatrixprofile 1.1.10 requires protobuf<4.0.0,>=3.11.2, but you have protobuf 4.24.4 which is incompatible.\ngcsfs 2021.11.0 requires fsspec==2021.11.0, but you have fsspec 2021.11.1 which is incompatible.\napache-beam 2.34.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.4 which is incompatible.\napache-beam 2.34.0 requires httplib2<0.20.0,>=0.8, but you have httplib2 0.20.2 which is incompatible.\napache-beam 2.34.0 requires protobuf<4,>=3.12.2, but you have protobuf 4.24.4 which is incompatible.\napache-beam 2.34.0 requires pyarrow<6.0.0,>=0.15.1, but you have pyarrow 6.0.0 which is incompatible.\u001b[0m\nSuccessfully installed d4rl-1.1 dm-control-1.0.13 dm-env-1.6 labmaze-1.0.6 mjrl-1.0.0 mujoco-2.3.6 protobuf-4.24.4 pybullet-3.2.6 pyopengl-3.1.7\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## check all the avilable environments","metadata":{}},{"cell_type":"code","source":"import gym\nimport d4rl\n\n# List all available environments to see if Maze2D is included\nenvs = gym.envs.registry.all()\nenv_names = [env_spec.id for env_spec in envs]\nprint(\"Available environments:\", env_names)","metadata":{"execution":{"iopub.status.busy":"2024-08-14T12:40:56.417020Z","iopub.execute_input":"2024-08-14T12:40:56.417323Z","iopub.status.idle":"2024-08-14T12:40:56.878405Z","shell.execute_reply.started":"2024-08-14T12:40:56.417282Z","shell.execute_reply":"2024-08-14T12:40:56.877689Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"Warning: Flow failed to import. Set the environment variable D4RL_SUPPRESS_IMPORT_ERROR=1 to suppress this message.\nNo module named 'flow'\n/opt/conda/lib/python3.7/site-packages/glfw/__init__.py:914: GLFWError: (65544) b'X11: The DISPLAY environment variable is missing'\n  warnings.warn(message, GLFWError)\nWarning: CARLA failed to import. Set the environment variable D4RL_SUPPRESS_IMPORT_ERROR=1 to suppress this message.\nNo module named 'carla'\npybullet build time: Nov 28 2023 23:50:19\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0xe but this version of numpy is 0xd"],"ename":"RuntimeError","evalue":"module compiled against API version 0xe but this version of numpy is 0xd","output_type":"error"},{"name":"stdout","text":"Available environments: ['CartPole-v0', 'CartPole-v1', 'MountainCar-v0', 'MountainCarContinuous-v0', 'Pendulum-v1', 'Acrobot-v1', 'LunarLander-v2', 'LunarLanderContinuous-v2', 'BipedalWalker-v3', 'BipedalWalkerHardcore-v3', 'CarRacing-v0', 'Blackjack-v1', 'FrozenLake-v1', 'FrozenLake8x8-v1', 'CliffWalking-v0', 'Taxi-v3', 'Reacher-v2', 'Pusher-v2', 'Thrower-v2', 'Striker-v2', 'InvertedPendulum-v2', 'InvertedDoublePendulum-v2', 'HalfCheetah-v2', 'HalfCheetah-v3', 'Hopper-v2', 'Hopper-v3', 'Swimmer-v2', 'Swimmer-v3', 'Walker2d-v2', 'Walker2d-v3', 'Ant-v2', 'Ant-v3', 'Humanoid-v2', 'Humanoid-v3', 'HumanoidStandup-v2', 'FetchSlide-v1', 'FetchPickAndPlace-v1', 'FetchReach-v1', 'FetchPush-v1', 'HandReach-v0', 'HandManipulateBlockRotateZ-v0', 'HandManipulateBlockRotateZTouchSensors-v0', 'HandManipulateBlockRotateZTouchSensors-v1', 'HandManipulateBlockRotateParallel-v0', 'HandManipulateBlockRotateParallelTouchSensors-v0', 'HandManipulateBlockRotateParallelTouchSensors-v1', 'HandManipulateBlockRotateXYZ-v0', 'HandManipulateBlockRotateXYZTouchSensors-v0', 'HandManipulateBlockRotateXYZTouchSensors-v1', 'HandManipulateBlockFull-v0', 'HandManipulateBlock-v0', 'HandManipulateBlockTouchSensors-v0', 'HandManipulateBlockTouchSensors-v1', 'HandManipulateEggRotate-v0', 'HandManipulateEggRotateTouchSensors-v0', 'HandManipulateEggRotateTouchSensors-v1', 'HandManipulateEggFull-v0', 'HandManipulateEgg-v0', 'HandManipulateEggTouchSensors-v0', 'HandManipulateEggTouchSensors-v1', 'HandManipulatePenRotate-v0', 'HandManipulatePenRotateTouchSensors-v0', 'HandManipulatePenRotateTouchSensors-v1', 'HandManipulatePenFull-v0', 'HandManipulatePen-v0', 'HandManipulatePenTouchSensors-v0', 'HandManipulatePenTouchSensors-v1', 'FetchSlideDense-v1', 'FetchPickAndPlaceDense-v1', 'FetchReachDense-v1', 'FetchPushDense-v1', 'HandReachDense-v0', 'HandManipulateBlockRotateZDense-v0', 'HandManipulateBlockRotateZTouchSensorsDense-v0', 'HandManipulateBlockRotateZTouchSensorsDense-v1', 'HandManipulateBlockRotateParallelDense-v0', 'HandManipulateBlockRotateParallelTouchSensorsDense-v0', 'HandManipulateBlockRotateParallelTouchSensorsDense-v1', 'HandManipulateBlockRotateXYZDense-v0', 'HandManipulateBlockRotateXYZTouchSensorsDense-v0', 'HandManipulateBlockRotateXYZTouchSensorsDense-v1', 'HandManipulateBlockFullDense-v0', 'HandManipulateBlockDense-v0', 'HandManipulateBlockTouchSensorsDense-v0', 'HandManipulateBlockTouchSensorsDense-v1', 'HandManipulateEggRotateDense-v0', 'HandManipulateEggRotateTouchSensorsDense-v0', 'HandManipulateEggRotateTouchSensorsDense-v1', 'HandManipulateEggFullDense-v0', 'HandManipulateEggDense-v0', 'HandManipulateEggTouchSensorsDense-v0', 'HandManipulateEggTouchSensorsDense-v1', 'HandManipulatePenRotateDense-v0', 'HandManipulatePenRotateTouchSensorsDense-v0', 'HandManipulatePenRotateTouchSensorsDense-v1', 'HandManipulatePenFullDense-v0', 'HandManipulatePenDense-v0', 'HandManipulatePenTouchSensorsDense-v0', 'HandManipulatePenTouchSensorsDense-v1', 'CubeCrash-v0', 'CubeCrashSparse-v0', 'CubeCrashScreenBecomesBlack-v0', 'MemorizeDigits-v0', 'antmaze-umaze-v0', 'antmaze-umaze-diverse-v0', 'antmaze-medium-play-v0', 'antmaze-medium-diverse-v0', 'antmaze-large-diverse-v0', 'antmaze-large-play-v0', 'antmaze-umaze-v1', 'antmaze-umaze-diverse-v1', 'antmaze-medium-play-v1', 'antmaze-medium-diverse-v1', 'antmaze-large-diverse-v1', 'antmaze-large-play-v1', 'antmaze-eval-umaze-v0', 'antmaze-eval-umaze-diverse-v0', 'antmaze-eval-medium-play-v0', 'antmaze-eval-medium-diverse-v0', 'antmaze-eval-large-diverse-v0', 'antmaze-eval-large-play-v0', 'antmaze-umaze-v2', 'antmaze-umaze-diverse-v2', 'antmaze-medium-play-v2', 'antmaze-medium-diverse-v2', 'antmaze-large-diverse-v2', 'antmaze-large-play-v2', 'mjrl_point_mass-v0', 'mjrl_swimmer-v0', 'mjrl_reacher_7dof-v0', 'mjrl_peg_insertion-v0', 'hammer-human-v1', 'hammer-human-longhorizon-v1', 'hammer-expert-v1', 'hammer-cloned-v1', 'pen-human-v1', 'pen-human-longhorizon-v1', 'pen-expert-v1', 'pen-cloned-v1', 'relocate-human-v1', 'relocate-human-longhorizon-v1', 'relocate-expert-v1', 'relocate-cloned-v1', 'door-human-v1', 'door-human-longhorizon-v1', 'door-expert-v1', 'door-cloned-v1', 'door-v0', 'door-human-v0', 'door-human-longhorizon-v0', 'door-cloned-v0', 'door-expert-v0', 'hammer-v0', 'hammer-human-v0', 'hammer-human-longhorizon-v0', 'hammer-cloned-v0', 'hammer-expert-v0', 'pen-v0', 'pen-human-v0', 'pen-human-longhorizon-v0', 'pen-cloned-v0', 'pen-expert-v0', 'relocate-v0', 'relocate-human-v0', 'relocate-human-longhorizon-v0', 'relocate-cloned-v0', 'relocate-expert-v0', 'maze2d-open-v0', 'maze2d-umaze-v0', 'maze2d-medium-v0', 'maze2d-large-v0', 'maze2d-umaze-v1', 'maze2d-medium-v1', 'maze2d-large-v1', 'maze2d-eval-umaze-v1', 'maze2d-eval-medium-v1', 'maze2d-eval-large-v1', 'maze2d-open-dense-v0', 'maze2d-umaze-dense-v0', 'maze2d-medium-dense-v0', 'maze2d-large-dense-v0', 'maze2d-umaze-dense-v1', 'maze2d-medium-dense-v1', 'maze2d-large-dense-v1', 'maze2d-eval-umaze-dense-v1', 'maze2d-eval-medium-dense-v1', 'maze2d-eval-large-dense-v1', 'minigrid-fourrooms-v0', 'minigrid-fourrooms-random-v0', 'hopper-random-v1', 'hopper-random-v2', 'hopper-medium-v1', 'hopper-medium-v2', 'hopper-expert-v1', 'hopper-expert-v2', 'hopper-medium-expert-v1', 'hopper-medium-expert-v2', 'hopper-medium-replay-v1', 'hopper-medium-replay-v2', 'hopper-full-replay-v1', 'hopper-full-replay-v2', 'halfcheetah-random-v1', 'halfcheetah-random-v2', 'halfcheetah-medium-v1', 'halfcheetah-medium-v2', 'halfcheetah-expert-v1', 'halfcheetah-expert-v2', 'halfcheetah-medium-expert-v1', 'halfcheetah-medium-expert-v2', 'halfcheetah-medium-replay-v1', 'halfcheetah-medium-replay-v2', 'halfcheetah-full-replay-v1', 'halfcheetah-full-replay-v2', 'ant-random-v1', 'ant-random-v2', 'ant-medium-v1', 'ant-medium-v2', 'ant-expert-v1', 'ant-expert-v2', 'ant-medium-expert-v1', 'ant-medium-expert-v2', 'ant-medium-replay-v1', 'ant-medium-replay-v2', 'ant-full-replay-v1', 'ant-full-replay-v2', 'walker2d-random-v1', 'walker2d-random-v2', 'walker2d-medium-v1', 'walker2d-medium-v2', 'walker2d-expert-v1', 'walker2d-expert-v2', 'walker2d-medium-expert-v1', 'walker2d-medium-expert-v2', 'walker2d-medium-replay-v1', 'walker2d-medium-replay-v2', 'walker2d-full-replay-v1', 'walker2d-full-replay-v2', 'hopper-medium-v0', 'halfcheetah-medium-v0', 'walker2d-medium-v0', 'hopper-expert-v0', 'halfcheetah-expert-v0', 'walker2d-expert-v0', 'hopper-random-v0', 'halfcheetah-random-v0', 'walker2d-random-v0', 'hopper-medium-replay-v0', 'walker2d-medium-replay-v0', 'halfcheetah-medium-replay-v0', 'walker2d-medium-expert-v0', 'halfcheetah-medium-expert-v0', 'hopper-medium-expert-v0', 'ant-medium-expert-v0', 'ant-medium-replay-v0', 'ant-medium-v0', 'ant-random-v0', 'ant-expert-v0', 'ant-random-expert-v0', 'kitchen_relax-v1', 'kitchen-complete-v0', 'kitchen-partial-v0', 'kitchen-mixed-v0', 'HumanoidDeepMimicBackflipBulletEnv-v1', 'HumanoidDeepMimicWalkBulletEnv-v1', 'CartPoleBulletEnv-v1', 'CartPoleContinuousBulletEnv-v0', 'MinitaurBulletEnv-v0', 'MinitaurBulletDuckEnv-v0', 'MinitaurExtendedEnv-v0', 'MinitaurReactiveEnv-v0', 'MinitaurBallGymEnv-v0', 'MinitaurTrottingEnv-v0', 'MinitaurStandGymEnv-v0', 'MinitaurAlternatingLegsEnv-v0', 'MinitaurFourLegStandEnv-v0', 'RacecarBulletEnv-v0', 'RacecarZedBulletEnv-v0', 'KukaBulletEnv-v0', 'KukaCamBulletEnv-v0', 'KukaDiverseObjectGrasping-v0', 'InvertedPendulumBulletEnv-v0', 'InvertedDoublePendulumBulletEnv-v0', 'InvertedPendulumSwingupBulletEnv-v0', 'ReacherBulletEnv-v0', 'PusherBulletEnv-v0', 'ThrowerBulletEnv-v0', 'Walker2DBulletEnv-v0', 'HalfCheetahBulletEnv-v0', 'AntBulletEnv-v0', 'HopperBulletEnv-v0', 'HumanoidBulletEnv-v0', 'HumanoidFlagrunBulletEnv-v0', 'HumanoidFlagrunHarderBulletEnv-v0']\n","output_type":"stream"},{"name":"stderr","text":"Warning: GymBullet failed to import. Set the environment variable D4RL_SUPPRESS_IMPORT_ERROR=1 to suppress this message.\nnumpy.core.multiarray failed to import\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## download the dataset for the hopper environment","metadata":{}},{"cell_type":"code","source":"\n# Create the environment\nenv = gym.make('hopper-expert-v2') #gym.make('maze2d-umaze-v1')\n\nprint(\"reset\",env.reset())\nprint(\"step\",env.step(env.action_space.sample()))\n\n#get the dataset for hopper\ndataset = env.get_dataset()\ndataset.keys()","metadata":{"execution":{"iopub.status.busy":"2024-08-14T12:40:56.879452Z","iopub.execute_input":"2024-08-14T12:40:56.879695Z","iopub.status.idle":"2024-08-14T12:41:10.997071Z","shell.execute_reply.started":"2024-08-14T12:40:56.879650Z","shell.execute_reply":"2024-08-14T12:41:10.996327Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/gym/spaces/box.py:74: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n  \"Box bound precision lowered by casting to {}\".format(self.dtype)\n","output_type":"stream"},{"name":"stdout","text":"reset [ 1.24684648e+00 -1.69293131e-03  3.63724685e-03 -4.98521525e-03\n -4.40750351e-03  4.68383935e-03  4.71364054e-03 -4.12849553e-03\n  1.00337904e-03 -1.37068981e-03 -4.51344860e-03]\nstep (array([ 1.2466744 , -0.00267417,  0.0035467 , -0.00658057, -0.00859457,\n       -0.05376307, -0.04783077, -0.23853173, -0.02048977, -0.39729633,\n       -1.04107604]), 0.9744432852570977, False, {})\nDownloading dataset: http://rail.eecs.berkeley.edu/datasets/offline_rl/gym_mujoco_v2/hopper_expert-v2.hdf5 to /root/.d4rl/datasets/hopper_expert-v2.hdf5\n","output_type":"stream"},{"name":"stderr","text":"load datafile: 100%|██████████| 21/21 [00:01<00:00, 15.86it/s]\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"dict_keys(['actions', 'infos/action_log_probs', 'infos/qpos', 'infos/qvel', 'metadata/algorithm', 'metadata/iteration', 'metadata/policy/fc0/bias', 'metadata/policy/fc0/weight', 'metadata/policy/fc1/bias', 'metadata/policy/fc1/weight', 'metadata/policy/last_fc/bias', 'metadata/policy/last_fc/weight', 'metadata/policy/last_fc_log_std/bias', 'metadata/policy/last_fc_log_std/weight', 'metadata/policy/nonlinearity', 'metadata/policy/output_distribution', 'next_observations', 'observations', 'rewards', 'terminals', 'timeouts'])"},"metadata":{}}]},{"cell_type":"code","source":" next_state, reward, done, info=env.step(env.action_space.sample())\nprint(next_state.shape)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-14T12:41:13.933585Z","iopub.execute_input":"2024-08-14T12:41:13.934325Z","iopub.status.idle":"2024-08-14T12:41:13.940003Z","shell.execute_reply.started":"2024-08-14T12:41:13.934285Z","shell.execute_reply":"2024-08-14T12:41:13.939118Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"(11,)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## let's check the dataset","metadata":{}},{"cell_type":"code","source":"print(env.observation_space.shape)\n#rename each cell of the state\nobsers= [\"torso_z\", \"torso_angle\", \"thigh_angle\", \"leg_angle\", \"foot_angle\", \"velocity_x\", \"velocity_z\", \"torso_ang_velocity\", \n         \"thigh_ang_velocity\", \"torso_ang_velocity\", \"foot_ang_velocity\"]\nprint(len(obsers))\n#print the dataset attributes\ndataset.keys()","metadata":{"execution":{"iopub.status.busy":"2024-08-14T12:41:17.567359Z","iopub.execute_input":"2024-08-14T12:41:17.568094Z","iopub.status.idle":"2024-08-14T12:41:17.575811Z","shell.execute_reply.started":"2024-08-14T12:41:17.568058Z","shell.execute_reply":"2024-08-14T12:41:17.575101Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"(11,)\n11\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"dict_keys(['actions', 'infos/action_log_probs', 'infos/qpos', 'infos/qvel', 'metadata/algorithm', 'metadata/iteration', 'metadata/policy/fc0/bias', 'metadata/policy/fc0/weight', 'metadata/policy/fc1/bias', 'metadata/policy/fc1/weight', 'metadata/policy/last_fc/bias', 'metadata/policy/last_fc/weight', 'metadata/policy/last_fc_log_std/bias', 'metadata/policy/last_fc_log_std/weight', 'metadata/policy/nonlinearity', 'metadata/policy/output_distribution', 'next_observations', 'observations', 'rewards', 'terminals', 'timeouts'])"},"metadata":{}}]},{"cell_type":"markdown","source":"### attributes of the dataset\n* actions: Actions taken by the agent.\n* infos/action_log_probs: Log probabilities of the taken actions.\n* infos/qpos: Generalized positions of the Hopper's joints and body parts.\n* infos/qvel: Generalized velocities of the Hopper's joints and body parts.\n* metadata/algorithm: The algorithm used to generate expert data.\n* metadata/iteration: Iteration number when the data was collected.\n* metadata/policy/fc0/bias: Bias of the first fully connected layer in the policy network.\n* metadata/policy/fc0/weight: Weights of the first fully connected layer in the policy network.\n* metadata/policy/fc1/bias: Bias of the second fully connected layer in the policy network.\n* metadata/policy/fc1/weight: Weights of the second fully connected layer in the policy network.\n* metadata/policy/last_fc/bias: Bias of the last fully connected layer in the policy network.\n* metadata/policy/last_fc/weight: Weights of the last fully connected layer in the policy network.\n* metadata/policy/last_fc_log_std/bias: Bias for the log standard deviation in the last layer.\n* metadata/policy/last_fc_log_std/weight: Weights for the log standard deviation in the last layer.\n* metadata/policy/nonlinearity: Type of nonlinearity used in the policy network.\n* metadata/policy/output_distribution: Distribution type for the policy's output.\n* next_observations: Observations after the agent takes an action.\n* observations: Observations before the agent takes an action.\n* rewards: Rewards received after actions are taken.\n* terminals: Indicates if an episode ended (True/False).\n* timeouts: Indicates if an episode ended due to a time limit (True/False).\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\n#unpack the dataset\nactions= dataset['actions'] # Actions taken by the agent\nobservations= dataset['observations'] # The observations or states seen by the agent.\nrewards= dataset['rewards'] #The reward received after each action. .reshape(-1, 1)\nnext_observations= dataset['next_observations'] #next observations after the agent takes an action\nterminals= dataset['terminals'] #Indicates if the episode ended in an unhealty way (1 if ended, 0 otherwise) \ntimeouts= dataset ['timeouts'] #Indicates if the episode ended due to a timeout.\ndones= np.logical_or(terminals, timeouts)\n\nprint(\"shapes of our columns/attributes: actions=\", actions.shape, \" observations=\", observations.shape, \" rewards=\", rewards.shape)\nprint(\"next_observations=\", next_observations.shape, \" terminals=\", terminals.shape,\" timeouts=\", timeouts.shape, \"done=\", dones.shape )","metadata":{"execution":{"iopub.status.busy":"2024-08-14T12:41:47.871619Z","iopub.execute_input":"2024-08-14T12:41:47.872359Z","iopub.status.idle":"2024-08-14T12:41:47.881161Z","shell.execute_reply.started":"2024-08-14T12:41:47.872322Z","shell.execute_reply":"2024-08-14T12:41:47.880465Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"shapes of our columns/attributes: actions= (1000000, 3)  observations= (1000000, 11)  rewards= (1000000,)\nnext_observations= (1000000, 11)  terminals= (1000000,)  timeouts= (1000000,) done= (1000000,)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"split dataset into train and test ","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nimport numpy as np\nimport torch\n\ntrain_observations, test_observations, train_actions, test_actions, train_rewards, test_rewards, train_next_observations, test_next_observations,train_dones, test_dones = train_test_split(\n    observations, actions, rewards, next_observations, dones, test_size=0.2, random_state=42\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-14T12:41:49.817871Z","iopub.execute_input":"2024-08-14T12:41:49.818614Z","iopub.status.idle":"2024-08-14T12:41:52.008225Z","shell.execute_reply.started":"2024-08-14T12:41:49.818573Z","shell.execute_reply":"2024-08-14T12:41:52.007511Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass CustomDataset(Dataset):\n    def __init__(self, observations, actions, next_observations, rewards, dones):\n        super().__init__()\n        # Extract data\n        self.observations = torch.tensor(observations, dtype=torch.float32)\n        self.actions = torch.tensor(actions, dtype=torch.float32)\n        self.rewards = torch.tensor(rewards, dtype=torch.float32)\n        self.next_observations = torch.tensor(next_observations, dtype=torch.float32)\n        self.dones= torch.tensor(dones,dtype= torch.bool )\n        #self.timeouts= torch.tensor(timeouts, dtype= torch.bool)\n        \n\n    def __len__(self):\n        return len(self.observations)\n\n    def __getitem__(self, idx):\n        return self.observations[idx], self.actions[idx], self.next_observations[idx], self.rewards[idx], self.dones[idx]\n\nwhole_dataset= CustomDataset( observations, actions, rewards, next_observations,  dones)\n\ntrain_set = CustomDataset(train_observations, train_actions, train_next_observations, train_rewards, train_dones)\ntest_set = CustomDataset(test_observations, test_actions, test_next_observations, test_rewards, test_dones)\nprint(len(train_set),len(test_set) , len(whole_dataset))","metadata":{"execution":{"iopub.status.busy":"2024-08-14T12:41:52.010163Z","iopub.execute_input":"2024-08-14T12:41:52.010740Z","iopub.status.idle":"2024-08-14T12:41:52.105843Z","shell.execute_reply.started":"2024-08-14T12:41:52.010695Z","shell.execute_reply":"2024-08-14T12:41:52.105058Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"800000 200000 1000000\n","output_type":"stream"}]},{"cell_type":"markdown","source":"create dataloaders","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nbatch_size = 120\ntrain_loader = DataLoader(train_set, batch_size, shuffle=True)\ntest_loader = DataLoader(test_set, batch_size, shuffle=False)\ndata_loader= DataLoader(whole_dataset, batch_size, shuffle= True)","metadata":{"execution":{"iopub.status.busy":"2024-08-14T12:41:52.107225Z","iopub.execute_input":"2024-08-14T12:41:52.107450Z","iopub.status.idle":"2024-08-14T12:41:52.112819Z","shell.execute_reply.started":"2024-08-14T12:41:52.107421Z","shell.execute_reply":"2024-08-14T12:41:52.111941Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# create the agent","metadata":{}},{"cell_type":"markdown","source":"### The actor responsible for choosing an action given a specific state \"policy network\"","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.distributions import Normal\n\nclass Actor(nn.Module):\n    def __init__(self, state_size, action_size, hidden_size= 32, standard_deviation_min= -10, standard_deviation_max= 10):\n        \"\"\"\n        state_size: (input size).\n        action_size: (output size).\n        hidden_size: Number of neurons in the hidden layers (default 32).\n        standard_deviation_min and standard_deviation_max: values used to clamp the log standard deviation of the policy's action distribution to prevent it from becoming too high or too low.\n        \"\"\"\n        super(Actor, self).__init__()\n        self.standard_deviation_min= standard_deviation_min\n        self.standard_deviation_max= standard_deviation_max\n        self.lin1= nn.Linear(state_size, hidden_size)\n        self.lin2= nn.Linear(hidden_size, hidden_size)\n        #output Linear layers to get the mean (mu) and the log standard deviation (log_sigma) of the action distribution.\n        self.mu= nn.Linear(hidden_size, action_size) # mean\n        self.log_sigma= nn.Linear(hidden_size, action_size) #standard deviation\n    \n    def forward(self, state):\n        x= F.relu(self.lin2(F.relu(self.lin1(state))))\n        \n        mu = torch.tanh(self.mu(x)) #The mean is passed through a tanh activation to keep the action within a certain range.\n        \n        log_sigma= self.log_sigma(x)\n        log_sigma = torch.clamp(log_sigma, self.standard_deviation_min, self.standard_deviation_max)\n        return mu, log_sigma\n    \n    def evaluate(self, state):\n        mu, log_sigma= self.forward(state) \n        sigma= log_sigma.exp()\n        normal_distribution= Normal(mu, sigma) #create normal distriution\n        \n        action= normal_distribution.sample() #get an action from the distriution\n        return action, normal_distribution \n    \n    def get_action(self, state): #to be used in the evaluation phase\n        mu, log_sigma= self.forward(state) \n        sigma= log_sigma.exp()\n        normal_distribution= Normal(mu, sigma) #create normal distriution\n        \n        action= normal_distribution.sample() #get an action from the distriution\n        return action.detach().cpu()\n    \n    def get_deterministic_action(self, state):\n        mu, _= self.forward(state)\n        return mu.detach().cpu()\n        \n        \n        \n        ","metadata":{"execution":{"iopub.status.busy":"2024-08-14T12:41:52.738687Z","iopub.execute_input":"2024-08-14T12:41:52.739004Z","iopub.status.idle":"2024-08-14T12:41:52.752360Z","shell.execute_reply.started":"2024-08-14T12:41:52.738964Z","shell.execute_reply":"2024-08-14T12:41:52.751583Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"### the critic estimates the Q-value, which is the expected return for taking a particular action in a given state.","metadata":{}},{"cell_type":"code","source":"class Critic(nn.Module):\n    def __init__(self, state_size, action_size, hidden_size= 32,seed= 1):\n        super(Critic, self).__init__()\n        torch.manual_seed(1)\n        self.lin1= nn.Linear(state_size+action_size, hidden_size)\n        self.lin2 = nn.Linear(hidden_size, hidden_size)\n        self.lin3= nn.Linear(hidden_size, 1)\n        \n    def forward(self, state, action):\n        #print(\"Critic\", state.shape, action.shape)\n        x= torch.cat((state, action), dim=-1)\n        #print(x.shape)\n        x= F.relu(self.lin2(F.relu(self.lin1(x))))\n        return self.lin3(x)","metadata":{"execution":{"iopub.status.busy":"2024-08-14T12:41:53.610037Z","iopub.execute_input":"2024-08-14T12:41:53.610735Z","iopub.status.idle":"2024-08-14T12:41:53.617955Z","shell.execute_reply.started":"2024-08-14T12:41:53.610688Z","shell.execute_reply":"2024-08-14T12:41:53.616927Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"###  the value estimates the expected return from a state, independent of any specific action.","metadata":{}},{"cell_type":"code","source":"class Value(nn.Module):\n    def __init__(self, state_size, hidden_size= 32):\n        super(Value, self).__init__()\n        self.lin1= nn.Linear(state_size, hidden_size)\n        self.lin2= nn.Linear(hidden_size, hidden_size)\n        self.lin3= nn.Linear(hidden_size, 1)\n        \n    def forward(self, state):\n        x= F.relu(self.lin2(F.relu(self.lin1(state))))\n        return self.lin3(x)","metadata":{"execution":{"iopub.status.busy":"2024-08-14T12:41:54.447685Z","iopub.execute_input":"2024-08-14T12:41:54.448187Z","iopub.status.idle":"2024-08-14T12:41:54.454290Z","shell.execute_reply.started":"2024-08-14T12:41:54.448153Z","shell.execute_reply":"2024-08-14T12:41:54.453506Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"### create the replay buffer","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport random\nimport torch\nfrom collections import deque, namedtuple\n\nclass ReplayBuffer:\n    def __init__(self, max_size, batch_size, device):\n        self.device= device\n        self.memory= deque(maxlen= max_size)\n        self.batch_size= batch_size\n        #define expericences tuple\n        self.experience= namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n        \n    def add(self, state, action, reward, next_state, done):\n        e= self.experience(state, action, reward, next_state, done)\n        self.memory.append(e)\n        \n    def sample(self):\n        #sample randome experiences from memory to train the model i=on it\n        experiences = random.sample(self.memory, k=self.batch_size)\n        #unpacked them into seperate tensors\n        states = torch.from_numpy(np.stack([e.state for e in experiences if e is not None])).float().to(self.device)\n        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(self.device)\n        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(self.device)\n        next_states = torch.from_numpy(np.stack([e.next_state for e in experiences if e is not None])).float().to(self.device)\n        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(self.device)\n        return (states, actions, rewards, next_states, terminals,timeouts )\n\n    def __len__(self):\n        return len(self.memory)","metadata":{"execution":{"iopub.status.busy":"2024-08-14T12:41:55.275762Z","iopub.execute_input":"2024-08-14T12:41:55.276023Z","iopub.status.idle":"2024-08-14T12:41:55.288747Z","shell.execute_reply.started":"2024-08-14T12:41:55.275993Z","shell.execute_reply":"2024-08-14T12:41:55.287958Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.optim as optim\nimport torch.nn as nn\nfrom torch.nn.utils import clip_grad_norm_\n\nclass Implicit_Q_learning(nn.Module):\n    def __init__(self, state_size, action_size, lr, hidden_size, tau, temperature, expectile, device):\n        super(Implicit_Q_learning, self).__init__()\n        self.state_size= state_size\n        self.action_size= action_size\n        self.lr= lr\n        self.hidden_size= hidden_size\n        self.tau= tau #update parameter for the target netwrok\n        self.device = device\n        self.temperature= torch.FloatTensor([temperature]).to(self.device)\n        self.expectile= torch.FloatTensor([expectile]).to(self.device)\n        self.gamma = torch.FloatTensor([0.99]).to(device) #to compute the Q-values\n        self.clip_grad_param = 1\n\n        #____define the neuralnetwroks____\n        # Actor_network: outputs actions based on the state.\n        self.actor_local= Actor(self.state_size, self.action_size, self.hidden_size).to(self.device)\n        self.actor_optimizer= optim.Adam(self.actor_local.parameters(), lr= self.lr)\n\n        # Critic_networks: Two Q-value networks to estimate the value of state-action pairs.\n        self.critic1= Critic(self.state_size, self.action_size, self.hidden_size, seed= 2).to(self.device)\n        self.critic2= Critic(self.state_size, self.action_size, self.hidden_size, seed= 1).to(self.device)\n        assert self.critic1.parameters() != self.critic2.parameters()\n\n        self.critic1_optimizer = optim.Adam(self.critic1.parameters(), lr=self.lr)\n        self.critic2_optimizer = optim.Adam(self.critic2.parameters(), lr=self.lr) \n\n        #Target networks for stability, which slowly track the critic networks.\n        self.critic1_target = Critic(self.state_size, self.action_size, self.hidden_size).to(self.device)\n        self.critic1_target.load_state_dict(self.critic1.state_dict())\n\n        self.critic2_target = Critic(self.state_size, self.action_size, self.hidden_size).to(self.device)\n        self.critic2_target.load_state_dict(self.critic2.state_dict())\n\n        #Value_network: estimates the value of a state independently of the action.\n        self.value_net = Value(self.state_size, self.hidden_size).to(self.device)\n        self.value_optimizer = optim.Adam(self.value_net.parameters(), lr=self.lr)\n        \n    def get_action(self, state, eval= False):\n        state= torch.from_numpy(state).float().to(self.device)\n        with torch.no_grad():\n            if eval:\n                action = self.actor_local.get_deterministic_action(state)\n            else:\n                action = self.actor_local.get_action(state)\n        return action.numpy()\n    \n    def calc_policy_loss(self, states, actions):\n        with torch.no_grad():\n            v = self.value_net(states)\n            q1 = self.critic1_target(states, actions)\n            q2 = self.critic2_target(states, actions)\n            min_Q = torch.min(q1,q2)\n\n        exp_a = torch.exp((min_Q - v) * self.temperature)\n        exp_a = torch.min(exp_a, torch.FloatTensor([100.0]).to(states.device))\n\n        _, dist = self.actor_local.evaluate(states)\n        log_probs = dist.log_prob(actions)\n        actor_loss = -(exp_a * log_probs).mean()\n\n        return actor_loss\n    \n    def calc_value_loss(self, states, actions):\n        with torch.no_grad():\n            q1 = self.critic1_target(states, actions)   \n            q2 = self.critic2_target(states, actions)\n            min_Q = torch.min(q1,q2)\n        \n        value = self.value_net(states)\n        value_loss = loss(min_Q - value, self.expectile).mean()\n        return value_loss\n    \n    def calc_q_loss(self, states, actions, rewards, dones, next_states):\n        with torch.no_grad():\n            next_v = self.value_net(next_states)\n            dones = dones.float()\n            q_target = rewards + (self.gamma * (1 - dones) * next_v) \n            #q_target = rewards + (self.gamma * (1 - terminals) * (1 - timeouts) * next_v)\n\n\n        q1 = self.critic1(states, actions)\n        q2 = self.critic2(states, actions)\n        critic1_loss = ((q1 - q_target)**2).mean() \n        critic2_loss = ((q2 - q_target)**2).mean()\n        return critic1_loss, critic2_loss\n\n\n    def learn(self, experiences):\n        states, actions, rewards, next_states,  dones  = experiences\n\n        self.value_optimizer.zero_grad()\n        #print(states, actions)\n        #print(states.shape, actions.shape)\n        value_loss = self.calc_value_loss(states, actions)\n        value_loss.backward()\n        self.value_optimizer.step()\n\n        actor_loss = self.calc_policy_loss(states, actions)\n        self.actor_optimizer.zero_grad()\n        actor_loss.backward()\n        self.actor_optimizer.step()\n        \n        critic1_loss, critic2_loss = self.calc_q_loss(states, actions, rewards, dones, next_states)\n\n        # critic 1\n        self.critic1_optimizer.zero_grad()\n        critic1_loss.backward()\n        clip_grad_norm_(self.critic1.parameters(), self.clip_grad_param)\n        self.critic1_optimizer.step()\n        # critic 2\n        self.critic2_optimizer.zero_grad()\n        critic2_loss.backward()\n        clip_grad_norm_(self.critic2.parameters(), self.clip_grad_param)\n        self.critic2_optimizer.step()\n\n        # ----------------------- update target networks ----------------------- #\n        self.soft_update(self.critic1, self.critic1_target)\n        self.soft_update(self.critic2, self.critic2_target)\n        \n        return actor_loss.item(), critic1_loss.item(), critic2_loss.item(), value_loss.item()\n\n    def soft_update(self, local_model , target_model):\n        \"\"\"Soft update model parameters.\n        θ_target = τ*θ_local + (1 - τ)*θ_target\n        Params\n        ======\n            local_model: PyTorch model (weights will be copied from)\n            target_model: PyTorch model (weights will be copied to)\n            tau (float): interpolation parameter \n        \"\"\"\n        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n            target_param.data.copy_(self.tau*local_param.data + (1.0-self.tau)*target_param.data)\n\n\n            \n            \n            \n            ","metadata":{"execution":{"iopub.status.busy":"2024-08-14T12:41:55.720231Z","iopub.execute_input":"2024-08-14T12:41:55.720493Z","iopub.status.idle":"2024-08-14T12:41:55.755400Z","shell.execute_reply.started":"2024-08-14T12:41:55.720458Z","shell.execute_reply":"2024-08-14T12:41:55.754713Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"#hyperparameters\nimport torch\nimport numpy as np\nimport random\n\n\nrun_name= \"IQL\"\nenv_name= \"hopper-expert-v2\" # environment name\nepisodes= 100 #number of episodes\nseed= 1 \nlog_video= 0  #\"Log agent behaviour to wanbd when set to 1, default: 0\"\nsave_every= 100 #save the network every x epochs\nbatch_size = 120\nhidden_size= 256\nlearning_rate= 3e-4\ntemperature= 3\nexpectile= 0.7\ntau= 5e-3\neval_every= 1\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n#define the train loader\ndataloader= DataLoader(whole_dataset, batch_size, shuffle= True)\n\n#the training loop\n#set the seeds\nnp.random.seed(seed)\nrandom.seed(seed)\ntorch.manual_seed(seed)\n#define an environment\nenv = gym.make('hopper-expert-v2') \n#set the seed for the action space\nenv.action_space.seed(seed)\n\nbatches = 0\naverage10 = deque(maxlen=10)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-14T12:41:56.246490Z","iopub.execute_input":"2024-08-14T12:41:56.246759Z","iopub.status.idle":"2024-08-14T12:41:56.338712Z","shell.execute_reply.started":"2024-08-14T12:41:56.246716Z","shell.execute_reply":"2024-08-14T12:41:56.337984Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/gym/spaces/box.py:74: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n  \"Box bound precision lowered by casting to {}\".format(self.dtype)\n","output_type":"stream"}]},{"cell_type":"code","source":"\"\"\"import torch\n\ndef save(args, save_name, model, wandb, ep=None):\n    import os\n    save_dir = './trained_models/' \n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir)\n    if not ep == None:\n        torch.save(model.state_dict(), save_dir + args.run_name + save_name + str(ep) + \".pth\")\n        wandb.save(save_dir + args.run_name + save_name + str(ep) + \".pth\")\n    else:\n        torch.save(model.state_dict(), save_dir + args.run_name + save_name + \".pth\")\n        wandb.save(save_dir + args.run_name + save_name + \".pth\")\n\ndef collect_random(env, dataset, num_samples=200):\n    state = env.reset()\n    for _ in range(num_samples):\n        action = env.action_space.sample()\n        next_state, reward, done, _ = env.step(action)\n        dataset.add(state, action, reward, next_state, done)\n        state = next_state\n        if done:\n            state = env.reset()\ndef loss(diff, expectile=0.8):\n    weight = torch.where(diff > 0, expectile, (1 - expectile))\n    return weight * (diff**2)\n    \"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-08-14T12:42:03.197972Z","iopub.execute_input":"2024-08-14T12:42:03.198235Z","iopub.status.idle":"2024-08-14T12:42:03.204558Z","shell.execute_reply.started":"2024-08-14T12:42:03.198207Z","shell.execute_reply":"2024-08-14T12:42:03.203796Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"'import torch\\n\\ndef save(args, save_name, model, wandb, ep=None):\\n    import os\\n    save_dir = \\'./trained_models/\\' \\n    if not os.path.exists(save_dir):\\n        os.makedirs(save_dir)\\n    if not ep == None:\\n        torch.save(model.state_dict(), save_dir + args.run_name + save_name + str(ep) + \".pth\")\\n        wandb.save(save_dir + args.run_name + save_name + str(ep) + \".pth\")\\n    else:\\n        torch.save(model.state_dict(), save_dir + args.run_name + save_name + \".pth\")\\n        wandb.save(save_dir + args.run_name + save_name + \".pth\")\\n\\ndef collect_random(env, dataset, num_samples=200):\\n    state = env.reset()\\n    for _ in range(num_samples):\\n        action = env.action_space.sample()\\n        next_state, reward, done, _ = env.step(action)\\n        dataset.add(state, action, reward, next_state, done)\\n        state = next_state\\n        if done:\\n            state = env.reset()\\ndef loss(diff, expectile=0.8):\\n    weight = torch.where(diff > 0, expectile, (1 - expectile))\\n    return weight * (diff**2)\\n    '"},"metadata":{}}]},{"cell_type":"code","source":"import torch\nimport os\nimport wandb\nimport random\n\ndef save(args, save_name, model, ep=None):\n    save_dir = './trained_models/' \n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir)\n    \n    save_path = os.path.join(save_dir, run_name + save_name)\n    if ep is not None:\n        save_path += str(ep) + \".pth\"\n    else:\n        save_path += \".pth\"\n    \n    torch.save(model.state_dict(), save_path)\n    wandb.save(save_path)\n\ndef collect_random(env, dataset, num_samples=200):\n    state = env.reset()\n    for _ in range(num_samples):\n        action = env.action_space.sample()\n        next_state, reward, termi, _ = env.step(action)\n        dataset.add(state, action, reward, next_state, done)\n        state = next_state\n        if terminals or timeout:\n            state = env.reset()\n\ndef loss(diff, expectile=0.8):\n    weight = torch.where(diff > 0, expectile, (1 - expectile))\n    return weight * (diff**2)\n\n'''\n\n# Simulate training\nepochs = 10\noffset = random.random() / 5\nfor epoch in range(2, epochs):\n    acc = 1 - 2 ** -epoch - random.random() / epoch - offset\n    loss_value = 2 ** -epoch + random.random() / epoch + offset\n\n    # Log metrics to WandB\n    if wandb:\n        wandb.log({\"acc\": acc, \"loss\": loss_value})\n\n# [optional] finish the WandB run, necessary in notebooks\nif wandb:\n    wandb.finish()\n'''","metadata":{"execution":{"iopub.status.busy":"2024-08-14T12:42:03.515770Z","iopub.execute_input":"2024-08-14T12:42:03.516288Z","iopub.status.idle":"2024-08-14T12:42:04.502100Z","shell.execute_reply.started":"2024-08-14T12:42:03.516256Z","shell.execute_reply":"2024-08-14T12:42:04.501272Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"'\\n\\n# Simulate training\\nepochs = 10\\noffset = random.random() / 5\\nfor epoch in range(2, epochs):\\n    acc = 1 - 2 ** -epoch - random.random() / epoch - offset\\n    loss_value = 2 ** -epoch + random.random() / epoch + offset\\n\\n    # Log metrics to WandB\\n    if wandb:\\n        wandb.log({\"acc\": acc, \"loss\": loss_value})\\n\\n# [optional] finish the WandB run, necessary in notebooks\\nif wandb:\\n    wandb.finish()\\n'"},"metadata":{}}]},{"cell_type":"code","source":"def evaluate(env, policy, eval_runs=5): \n    \"\"\"\n    Makes an evaluation run with the current policy\n    \"\"\"\n    reward_batch = []\n    for i in range(eval_runs):\n        state = env.reset()\n\n        rewards = 0\n        while True:\n            action = policy.get_action(state, eval=True)\n\n            state, reward, done, _ = env.step(action)\n            rewards += reward\n            if done:\n                break\n        reward_batch.append(rewards)\n    return np.mean(reward_batch)","metadata":{"execution":{"iopub.status.busy":"2024-08-14T12:42:04.504238Z","iopub.execute_input":"2024-08-14T12:42:04.504980Z","iopub.status.idle":"2024-08-14T12:42:04.511092Z","shell.execute_reply.started":"2024-08-14T12:42:04.504933Z","shell.execute_reply":"2024-08-14T12:42:04.510275Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"! pip install protobuf==3.20.0","metadata":{"execution":{"iopub.status.busy":"2024-08-14T12:42:04.545141Z","iopub.execute_input":"2024-08-14T12:42:04.545359Z","iopub.status.idle":"2024-08-14T12:42:14.943924Z","shell.execute_reply.started":"2024-08-14T12:42:04.545332Z","shell.execute_reply":"2024-08-14T12:42:14.942991Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Collecting protobuf==3.20.0\n  Downloading protobuf-3.20.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n     |████████████████████████████████| 1.0 MB 1.1 MB/s            \n\u001b[?25hInstalling collected packages: protobuf\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 4.24.4\n    Uninstalling protobuf-4.24.4:\n      Successfully uninstalled protobuf-4.24.4\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-io 0.21.0 requires tensorflow-io-gcs-filesystem==0.21.0, which is not installed.\nexplainable-ai-sdk 1.3.2 requires xai-image-widget, which is not installed.\ncudf 21.10.1 requires cupy-cuda110, which is not installed.\nbeatrix-jupyterlab 3.1.4 requires google-cloud-bigquery-storage, which is not installed.\ntfx-bsl 1.4.0 requires absl-py<0.13,>=0.9, but you have absl-py 0.15.0 which is incompatible.\ntfx-bsl 1.4.0 requires pyarrow<6,>=1, but you have pyarrow 6.0.0 which is incompatible.\ntensorflow 2.6.2 requires six~=1.15.0, but you have six 1.16.0 which is incompatible.\ntensorflow 2.6.2 requires typing-extensions~=3.7.4, but you have typing-extensions 3.10.0.2 which is incompatible.\ntensorflow 2.6.2 requires wrapt~=1.12.1, but you have wrapt 1.13.3 which is incompatible.\ntensorflow-transform 1.4.0 requires absl-py<0.13,>=0.9, but you have absl-py 0.15.0 which is incompatible.\ntensorflow-transform 1.4.0 requires pyarrow<6,>=1, but you have pyarrow 6.0.0 which is incompatible.\ntensorflow-metadata 1.4.0 requires absl-py<0.13,>=0.9, but you have absl-py 0.15.0 which is incompatible.\ngcsfs 2021.11.0 requires fsspec==2021.11.0, but you have fsspec 2021.11.1 which is incompatible.\napache-beam 2.34.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.4 which is incompatible.\napache-beam 2.34.0 requires httplib2<0.20.0,>=0.8, but you have httplib2 0.20.2 which is incompatible.\napache-beam 2.34.0 requires pyarrow<6.0.0,>=0.15.1, but you have pyarrow 6.0.0 which is incompatible.\u001b[0m\nSuccessfully installed protobuf-3.20.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install wandb","metadata":{"execution":{"iopub.status.busy":"2024-08-14T12:42:49.860407Z","iopub.execute_input":"2024-08-14T12:42:49.860744Z","iopub.status.idle":"2024-08-14T12:42:58.625932Z","shell.execute_reply.started":"2024-08-14T12:42:49.860706Z","shell.execute_reply":"2024-08-14T12:42:58.625103Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Requirement already satisfied: wandb in /opt/conda/lib/python3.7/site-packages (0.12.7)\nRequirement already satisfied: shortuuid>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (1.0.8)\nRequirement already satisfied: GitPython>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (3.1.24)\nRequirement already satisfied: promise<3,>=2.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (2.3)\nRequirement already satisfied: six>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (1.16.0)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (5.8.0)\nRequirement already satisfied: subprocess32>=3.5.3 in /opt/conda/lib/python3.7/site-packages (from wandb) (3.5.4)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.7/site-packages (from wandb) (6.0)\nRequirement already satisfied: configparser>=3.8.1 in /opt/conda/lib/python3.7/site-packages (from wandb) (5.1.0)\nRequirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.7/site-packages (from wandb) (2.8.0)\nRequirement already satisfied: Click!=8.0.0,>=7.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (8.0.3)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: pathtools in /opt/conda/lib/python3.7/site-packages (from wandb) (0.1.2)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (2.25.1)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (1.5.0)\nRequirement already satisfied: yaspin>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (2.1.0)\nRequirement already satisfied: protobuf>=3.12.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (3.20.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from Click!=8.0.0,>=7.0->wandb) (4.8.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from GitPython>=1.0.0->wandb) (3.10.0.2)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.7/site-packages (from GitPython>=1.0.0->wandb) (4.0.9)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (4.0.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (1.26.7)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (2.10)\nRequirement already satisfied: termcolor<2.0.0,>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from yaspin>=1.0.0->wandb) (1.1.0)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (3.0.5)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->Click!=8.0.0,>=7.0->wandb) (3.6.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"import wandb\nwandb.login()","metadata":{"execution":{"iopub.status.busy":"2024-08-14T12:42:58.628123Z","iopub.execute_input":"2024-08-14T12:42:58.628871Z","iopub.status.idle":"2024-08-14T12:43:04.005130Z","shell.execute_reply.started":"2024-08-14T12:42:58.628822Z","shell.execute_reply":"2024-08-14T12:43:04.004443Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"# Start a W&B Run with wandb.init\nrun = wandb.init(project=\"offline-RL\", config={\"architecture\": \"IQL\",\"dataset\": \"hopper-expert-v2\"})\nif run:\n    print(\"state_size= \", env.observation_space.shape[0])\n    print(\"action_size= \", env.action_space.shape[0])\n    agent = Implicit_Q_learning(state_size=env.observation_space.shape[0],\n            action_size=env.action_space.shape[0],\n            lr=learning_rate,\n            hidden_size=hidden_size,\n            tau=tau,\n            temperature=temperature,\n            expectile=expectile,\n            device=device)\n    run.watch(agent, log=\"gradients\", log_freq=10)\n    if log_video:\n        env = gym.wrappers.Monitor(env, './video', video_callable=lambda x: x%10==0, force=True)\n    eval_reward = evaluate(env, agent)\n    run.log({\"Test Reward\": eval_reward, \"Episode\": 0, \"Batches\": batches}, step=batches)\n    for i in range(1, episodes+1):\n        for batch_idx, experience in enumerate(dataloader):\n            states, actions, rewards, next_states, dones = experience\n            states = states.to(device)\n            actions = actions.to(device)\n            rewards = rewards.to(device)\n            next_states = next_states.to(device)\n            dones = dones.to(device)\n            policy_loss, critic1_loss, critic2_loss, value_loss = agent.learn((states, actions, rewards, next_states, dones))\n            batches += 1\n            \n        if i % eval_every == 0:\n            eval_reward = evaluate(env, agent)\n            run.log({\"Test Reward\": eval_reward, \"Episode\": i, \"Batches\": batches}, step=batches)\n\n            average10.append(eval_reward)\n            print(\"Episode: {} | Reward: {} | Polciy Loss: {} | Batches: {}\".format(i, eval_reward, policy_loss, batches))\n            \n        run.log({\n            \"Average10\": np.mean(average10),\n            \"Policy Loss\": policy_loss,\n            \"Value Loss\": value_loss,\n            \"Critic 1 Loss\": critic1_loss,\n            \"Critic 2 Loss\": critic2_loss,\n            \"Batches\": batches,\n            \"Episode\": i})\n        if (i %10 == 0) and log_video:\n            mp4list = glob.glob('video/*.mp4')\n            if len(mp4list) > 1:\n                mp4 = mp4list[-2]\n                run.log({\"gameplays\": run.Video(mp4, caption='episode: '+str(i-10), fps=4, format=\"gif\"), \"Episode\": i})\n            \n            \n  \n","metadata":{"execution":{"iopub.status.busy":"2024-08-14T12:43:06.384810Z","iopub.execute_input":"2024-08-14T12:43:06.385577Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33manwar96ibrahim-student\u001b[0m (use `wandb login --relogin` to force relogin)\n\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.6 is available!  To upgrade, please run:\n\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n                    Syncing run <strong><a href=\"https://wandb.ai/anwar96ibrahim-student/offline-RL/runs/lkocdryy\" target=\"_blank\">deft-shape-23</a></strong> to <a href=\"https://wandb.ai/anwar96ibrahim-student/offline-RL\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n\n                "},"metadata":{}},{"name":"stdout","text":"state_size=  11\naction_size=  3\nEpisode: 1 | Reward: 88.87086824568685 | Polciy Loss: -0.09152543544769287 | Batches: 8334\n","output_type":"stream"}]},{"cell_type":"code","source":"\"\"\"import gym\nimport d4rl\nimport numpy as np\nfrom collections import deque\nimport torch\nimport wandb\nimport argparse\nimport glob\n#from utils import save, collect_random\nimport random\n#from agent import IQL\nfrom torch.utils.data import DataLoader, TensorDataset\n\n\ndef get_config():\n    parser = argparse.ArgumentParser(description='RL')\n    parser.add_argument(\"--run_name\", type=str, default=\"IQL\", help=\"Run name, default: SAC\")\n    parser.add_argument(\"--env\", type=str, default=\"halfcheetah-medium-v2\", help=\"Gym environment name, default: Pendulum-v0\") #\"hopper-expert-v2\"\n    parser.add_argument(\"--episodes\", type=int, default=100, help=\"Number of episodes, default: 100\")\n    parser.add_argument(\"--seed\", type=int, default=1, help=\"Seed, default: 1\")\n    parser.add_argument(\"--log_video\", type=int, default=0, help=\"Log agent behaviour to wanbd when set to 1, default: 0\")\n    parser.add_argument(\"--save_every\", type=int, default=100, help=\"Saves the network every x epochs, default: 25\")\n    parser.add_argument(\"--batch_size\", type=int, default=512, help=\"Batch size, default: 256\")\n    parser.add_argument(\"--hidden_size\", type=int, default=256, help=\"\")\n    parser.add_argument(\"--learning_rate\", type=float, default=3e-4, help=\"\")\n    parser.add_argument(\"--temperature\", type=float, default=3, help=\"\")\n    parser.add_argument(\"--expectile\", type=float, default=0.7, help=\"\")\n    parser.add_argument(\"--tau\", type=float, default=5e-3, help=\"\")\n    parser.add_argument(\"--eval_every\", type=int, default=1, help=\"\")\n    \n    args = parser.parse_args()\n    return args\n\ndef prep_dataloader(env_id=\"halfcheetah-medium-v2\", batch_size=256, seed=1):\n    env = gym.make(env_id)\n    dataset = env.get_dataset()\n    tensors = {}\n    for k, v in dataset.items():\n        if k in [\"actions\", \"observations\", \"next_observations\", \"rewards\", \"terminals\"]:\n            if  k is not \"terminals\":\n                tensors[k] = torch.from_numpy(v).float()\n            else:\n                tensors[k] = torch.from_numpy(v).long()\n    \n    tensordata = TensorDataset(tensors[\"observations\"],\n                               tensors[\"actions\"],\n                               tensors[\"rewards\"],\n                               tensors[\"next_observations\"],\n                               tensors[\"terminals\"])\n    dataloader  = DataLoader(tensordata, batch_size=batch_size, shuffle=True)\n    \n    if \"halfcheetah\" in env_id:\n        eval_env = gym.make(\"HalfCheetah-v2\")\n    eval_env.seed(seed)\n    return dataloader, eval_env\n\ndef evaluate(env, policy, eval_runs=5): \n    \"\"\"\n    Makes an evaluation run with the current policy\n    \"\"\"\n    reward_batch = []\n    for i in range(eval_runs):\n        state = env.reset()\n\n        rewards = 0\n        while True:\n            action = policy.get_action(state, eval=True)\n\n            state, reward, done, _ = env.step(action)\n            rewards += reward\n            if done:\n                break\n        reward_batch.append(rewards)\n    return np.mean(reward_batch)\n\ndef train(config):\n    np.random.seed(config.seed)\n    random.seed(config.seed)\n    torch.manual_seed(config.seed)\n\n    #data_loader= DataLoader(whole_dataset, batch_size, shuffle= True)\n    dataloader, env = prep_dataloader(env_id=config.env, batch_size=config.batch_size, seed=config.seed)\n\n    env.action_space.seed(config.seed)\n\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    \n    batches = 0\n    average10 = deque(maxlen=10)\n    \n    with wandb.init(project=\"IQL-offline\", name=config.run_name, config=config):\n        \n        agent = IQL(state_size=env.observation_space.shape[0],\n                    action_size=env.action_space.shape[0],\n                    learning_rate=config.learning_rate,\n                    hidden_size=config.hidden_size,\n                    tau=config.tau,\n                    temperature=config.temperature,\n                    expectile=config.expectile,\n                    device=device)\n\n        wandb.watch(agent, log=\"gradients\", log_freq=10)\n        if config.log_video:\n            env = gym.wrappers.Monitor(env, './video', video_callable=lambda x: x%10==0, force=True)\n        eval_reward = evaluate(env, agent)\n        wandb.log({\"Test Reward\": eval_reward, \"Episode\": 0, \"Batches\": batches}, step=batches)\n        for i in range(1, config.episodes+1):\n\n            for batch_idx, experience in enumerate(dataloader):\n                states, actions, rewards, next_states, dones = experience\n                states = states.to(device)\n                actions = actions.to(device)\n                rewards = rewards.to(device)\n                next_states = next_states.to(device)\n                dones = dones.to(device)\n                policy_loss, critic1_loss, critic2_loss, value_loss = agent.learn((states, actions, rewards, next_states, dones))\n                batches += 1\n\n            if i % config.eval_every == 0:\n                eval_reward = evaluate(env, agent)\n                wandb.log({\"Test Reward\": eval_reward, \"Episode\": i, \"Batches\": batches}, step=batches)\n\n                average10.append(eval_reward)\n                print(\"Episode: {} | Reward: {} | Polciy Loss: {} | Batches: {}\".format(i, eval_reward, policy_loss, batches,))\n            \n            wandb.log({\n                       \"Average10\": np.mean(average10),\n                       \"Policy Loss\": policy_loss,\n                       \"Value Loss\": value_loss,\n                       \"Critic 1 Loss\": critic1_loss,\n                       \"Critic 2 Loss\": critic2_loss,\n                       \"Batches\": batches,\n                       \"Episode\": i})\n\n            if (i %10 == 0) and config.log_video:\n                mp4list = glob.glob('video/*.mp4')\n                if len(mp4list) > 1:\n                    mp4 = mp4list[-2]\n                    wandb.log({\"gameplays\": wandb.Video(mp4, caption='episode: '+str(i-10), fps=4, format=\"gif\"), \"Episode\": i})\n\n            if i % config.save_every == 0:\n                save(config, save_name=\"IQL\", model=agent.actor_local, wandb=wandb, ep=0)\n\nif __name__ == \"__main__\":\n    config = get_config()\n    train(config)\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"first we will unpack it","metadata":{}},{"cell_type":"markdown","source":"# **Step 2 :** Install Virtual Display","metadata":{}},{"cell_type":"code","source":"!apt install -y python-opengl ffmpeg > /dev/null 2>&1\n\n# !apt install -y xvfb\n%pip install pyvirtualdisplay","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then import it and define some helper functions:","metadata":{}},{"cell_type":"code","source":"from pyvirtualdisplay import Display\ndisplay = Display(visible=0, size=(400, 300))\ndisplay.start()\n\n\nfrom matplotlib import pyplot as plt, animation\n%matplotlib inline\nfrom IPython import display\n\ndef create_anim(frames, dpi, fps):\n    plt.figure(figsize=(frames[0].shape[1] / dpi, frames[0].shape[0] / dpi), dpi=dpi)\n    patch = plt.imshow(frames[0])\n    def setup():\n        plt.axis('off')\n    def animate(i):\n        patch.set_data(frames[i])\n    anim = animation.FuncAnimation(plt.gcf(), animate, init_func=setup, frames=len(frames), interval=fps)\n    return anim\n\ndef display_anim(frames, dpi=10, fps=100):\n    anim = create_anim(frames, dpi, fps)\n    return anim.to_jshtml()\n\ndef save_anim(frames, filename, dpi=60, fps=50):\n    anim = create_anim(frames, dpi, fps)\n    anim.save(filename)\n\n\nclass trigger:\n    def __init__(self):\n        self._trigger = True\n\n    def __call__(self, e):\n        return self._trigger\n\n    def set(self, t):\n        self._trigger = t","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Step 3 :** import OpenAI gym , Install Tensorflow 1.x and Stable baselines 2.x","metadata":{}},{"cell_type":"code","source":"import gym\n\n!pip install tensorflow==1.15.0\nimport tensorflow as tf\n\n!apt-get install -y cmake libopenmpi-dev python3-dev zlib1g-dev\n!pip install \"stable-baselines[mpi]==2.9.0\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Step 4 :** Create The Agent Model And Train It","metadata":{}},{"cell_type":"markdown","source":"Create our environment , and the model (I will use Walker2d-v2 environment but you can change it to any mujuco environments available in OpenAI gym) :","metadata":{}},{"cell_type":"code","source":"from stable_baselines import PPO1\nfrom stable_baselines.common.policies import MlpPolicy\nenv = gym.make(\"Walker2d-v2\")\n\nmodel = PPO1(MlpPolicy, env, verbose=0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Train our agent :","metadata":{}},{"cell_type":"code","source":"model.learn(total_timesteps=4000000)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Step 5 :** Display Episode Frames In the notebook","metadata":{}},{"cell_type":"markdown","source":"Finally render an episode and display it in the notebook :","metadata":{}},{"cell_type":"code","source":"frames = []\nenv = gym.make(\"Walker2d-v2\")\nobs = env.reset()\ndone = False\nepisode_reward = 0\nwhile not done:\n    frames.append(env.render(mode='rgb_array'))\n    obs, rew,done,info = env.step(model.predict(obs)[0])\n    episode_reward += rew\nenv.close()\n\nprint(\"Episode Reward: \" + str(episode_reward))\n\ndisplay.HTML(display_anim(frames))","metadata":{},"execution_count":null,"outputs":[]}]}